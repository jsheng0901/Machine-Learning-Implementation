
### 基本原理
参考链接：
- [中文原理讲解及例子解释XGB](https://zhuanlan.zhihu.com/p/83901304)
- [中文解释集成学习系列总结之XGB](https://www.cnblogs.com/massquantity/p/9794480.html)
- [xgb官方文档以及各方面详细应用解释](https://xgboost.readthedocs.io/en/stable/index.html)
### 优点
- 精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数。
- 灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导。
- 正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是XGBoost优于传统GBDT的一个特性。
- 列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样及特征抽样，不仅能降低过拟合，还能减少计算。这也是XGBoost异于传统GBDT的一个特性。
- 缺失值处理：对于特征的值有缺失的样本，XGBoost 采用的稀疏感知算法可以自动学习出它的分裂方向。在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销。在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向。
- XGBoost工具支持并行：注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- 可并行的近似算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，考虑自定义带权重版本的分位发，用于高效地生成候选的分割点。
- 传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的。 不过并不觉得这是“纯粹”的后剪枝，因为一般还是要预先限制最大深度。
- 其它优点继承GBDT所有优点。XGB更像是GBDT在大数据工程下加速并且精度，抗噪能力都提升的版本。
### 缺点
- 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集。
- 对比随机森林，RF的起始性能较差，特别当只有一个基学习器时，但随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。RF一般降低variance，而XGB一般降低bias。即使有正则化，随着树变多还是很容易过拟合对比RF。
- 虽然有预排序来提速找最佳分裂点，但预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。
### 知识点提炼
- 参数解释
  - n_estimators，构建多少个并行的DT，一般来说XGB每棵树都很简单，所以总共多少棵树一般选择比较大的值比如200，然后配合early stopping防止过拟合。
  - max_depth，单个决策树的最大深度，和减枝有关，如果不设置，则一直build树到底。这个参数很重要对于XGB，因为我们会构建很多树，所以单个树越简单越好来避免过拟合，一般设置4左右。
  - min_samples_split，单个决策树分裂后子树最小需要多少个样本才可以去分裂一个树节点。
  - min_samples_leaf，单个决策树节点构建叶子结点的时候，叶子结点最少需要多少个样本来构建。如果分裂后低于这个参数则意味着不再分裂此节点。此参数可能会影响回归任务的平滑效果。
  - min_impurity_decrease，单个决策树节点最小需要降低多少impurity，如果分裂后低于此参数则意味着不再分裂此节点。此参数可以达到early stopping的效果，防止过拟合太多的单一决策树。
  - learning_rate，每个单一决策树对于最终结果的贡献权重，缩小单一树对于最终结果的共享。和n_estimators参数是一组相互牵制的参数，相互之间有trade-off。
  - loss，每次拟合优化的损失函数，对于不同的任务和不同的数据分布，我们应该采用不同的损失函数。
  - tol，early stopping的忍受度，如果loss下降幅度在几轮构建DT中都低于此参数的时候，我们停止训练，及early stopping。
  - subsample，数值从 0 - 1， 是一个ratio，每次构建树的时候用的样本个数的比例，比如0.5以为的随机使用一般的数据构建树。
  - col_sample_by*，也叫做列抽样，借鉴RF里面的参数，数值从 0 - 1， 是一个ratio。by*这里可以选择by_tree，by_level，by_node，分别对应构建每棵树，每一层树或者每一个节点时随机选择特征的比例。
- 损失函数解释
  - 损失函数可以是自定义的，只要满足符合一阶导数和二阶导数即可。
  - 回归问题：
    1. 均方差损失：(y-f(x))^2，适用于大部分数据分布均匀的情况。
    2. Huber损失：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。
    3. 分位数损失：对训练集进行分段预测的时候，则采用quantile。
  - 分类问题：
    1. 指数损失：exp(-yF(x))，类似Adaboost。
    2. 对数损失：交叉嫡损失，主要用于二分和多分类问题。
- 正则化解释
  - Shrinkage，对应参数里面的learning_rate，及我们每次对每棵树的拟合进行缩放。实践中一般把此参数设置很小，比如0.01之类的，然后配合early stopping来使用。
  - Subsample，子采样，取值为 (0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做XGB的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但会增加样本拟合的偏差，因此取值不能太低。推荐在 [0.5, 0.8]之间。
  - Early Stopping，对应参数里面的tol，机器学习迭代式训练模型中很常见的防止过拟合技巧，具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。在sklearn的GBDT中可以设置参数n_iter_no_change和tol实现early stopping。
  - 与GBDT最大的不同是XGB在目标函数里面就加入了正则化，正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。这样每次增加分裂都能有一定的penalty，有效的控制了树的复杂度。
  - 列抽样也能有效的降低每次树拟合的复杂度，达到降低variance的效果。
  - 其它正则化的方法就是对每次拟合的单一CART树进行限制，此处参考decision tree里面的note详细介绍了单一树的正则化。
- 其它要点 
  - 子采样，借用bootstrap的思想，每一轮训练时只使用一部分样本，不同点是这里的采样是无放回抽样，这个方法被称为Stochastic Gradient Boosting。对于单棵树来说，只使用一部分样本拟合会增加单棵树的偏差和方差，然而subsampling会使树与树之间的相关性减少，从而降低模型的整体方差，很多时候会提高准确性。subsampling的另一个好处是因为只使用一部分样本进行训练，所以能显著降低计算开销。
  - 大部分单一树的要点可以参考decision tree里面的note。
  - 总的来说，对于XGB build的时间复杂度一般是O(n * log(n) * n_estimators)，predict的时间复杂度是O(log(n) * n_estimators)，n是样本个数，也就是每科单一树的时间乘以总共几棵树，这里和GBDT最大的不一样是支持feature直接并行运算找最佳分裂特征。
  - 除了以上最大的不同外，同时还有近似算法，稀疏感知，工程上用列块并行提前存储排序后的特征值和一阶导数二阶导数信息，缓存访问以及核外块计算利用硬盘加独立的线程加速读取数据等工程上的优化。
  - XGB对比RF可参考GBDT里面的note详细介绍了各方面的对比。
  - XGB使用二阶导数信息，因为二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。
  - 总的来说，XGB是大数据工程实现并提高精度和降低方差能力版本的GBDT。
### Engineer Work
- 一般来说直接调用sklearn中的XGB接口来实现。实际生产中普通版本目前来说在(440, 10)的数据集上每棵树拟合需要6s左右在8cores/16Memory上。
- 如需要再1M以上的数据跑production，目前本人测试过的情况是在GCP上1M的数据还是很容易实现进pipeline的。
- 如果需要跑更大的数据，推荐参考spark版本带GPU的XGB。[参考link](https://xgboost.readthedocs.io/en/stable/tutorials/spark_estimator.html#xgboost-pyspark-estimator)