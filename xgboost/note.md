
### 基本原理
参考链接：
- [中文原理讲解及例子解释XGB](https://zhuanlan.zhihu.com/p/83901304)
- [中文解释集成学习系列总结之XGB](https://www.cnblogs.com/massquantity/p/9794480.html)
- [xgb官方文档以及各方面详细应用解释](https://xgboost.readthedocs.io/en/stable/index.html)
### 优点
- 精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数。
- 灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导。
- 正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是XGBoost优于传统GBDT的一个特性。
- 列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样及特征抽样，不仅能降低过拟合，还能减少计算。这也是XGBoost异于传统GBDT的一个特性。
- 缺失值处理：对于特征的值有缺失的样本，XGBoost 采用的稀疏感知算法可以自动学习出它的分裂方向。在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销。在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。
- XGBoost工具支持并行：注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- 可并行的近似算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，考虑自定义带权重版本的分位发，用于高效地生成候选的分割点。
- 传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的。 不过并不觉得这是“纯粹”的后剪枝，因为一般还是要预先限制最大深度。
- 其它优点继承GBDT所有优点。XGB更像是GBDT在大数据工程下加速并且精度，抗噪能力都提升的版本。
### 缺点
- 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集。
- 对比随机森林，RF的起始性能较差，特别当只有一个基学习器时，但随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。RF一般降低variance，而XGB一般降低bias。即使有正则化，随着树变多还是很容易过拟合对比RF。
- 虽然有预排序来提速找最佳分裂点，但预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。
### 知识点提炼
- 参数解释
  - n_estimators，构建多少个并行的DT，一般来说XGB每棵树都很简单，所以总共多少棵树一般选择比较大的值比如200，然后配合early stopping防止过拟合。
  - max_depth，单个决策树的最大深度，和减枝有关，如果不设置，则一直build树到底。这个参数很重要对于XGB，因为我们会构建很多树，所以单个树越简单越好来避免过拟合，一般设置4左右。
  - min_samples_split，单个决策树分裂后子树最小需要多少个样本才可以去分裂一个树节点。
  - min_samples_leaf，单个决策树节点构建叶子结点的时候，叶子结点最少需要多少个样本来构建。如果分裂后低于这个参数则意味着不再分裂此节点。此参数可能会影响回归任务的平滑效果。
  - min_impurity_decrease，单个决策树节点最小需要降低多少impurity，如果分裂后低于此参数则意味着不再分裂此节点。此参数可以达到early stopping的效果，防止过拟合太多的单一决策树。
  - learning_rate，每个单一决策树对于最终结果的贡献权重，缩小单一树对于最终结果的共享。和n_estimators参数是一组相互牵制的参数，相互之间有trade-off。
  - loss，每次拟合优化的损失函数，对于不同的任务和不同的数据分布，我们应该采用不同的损失函数。
  - tol，early stopping的忍受度，如果loss下降幅度在几轮构建DT中都低于此参数的时候，我们停止训练，及early stopping。
  - subsample，数值从 0 - 1， 是一个ratio，每次构建树的时候用的样本个数的比例，比如0.5以为的随机使用一半的数据构建树。
  - col_sample_by*，也叫做列抽样，借鉴RF里面的参数，数值从 0 - 1， 是一个ratio。by*这里可以选择by_tree，by_level，by_node，分别对应构建每棵树，每一层树或者每一个节点时随机选择特征的比例。
- 损失函数解释
  - 损失函数可以是自定义的，只要满足符合一阶导数和二阶导数即可。
  - 回归问题：
    1. 均方差损失：(y-f(x))^2，适用于大部分数据分布均匀的情况。
    2. Huber损失：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。
    3. 分位数损失：对训练集进行分段预测的时候，则采用quantile。
  - 分类问题：
    1. 指数损失：exp(-yF(x))，类似Adaboost。
    2. 对数损失：交叉嫡损失，主要用于二分和多分类问题。
- 正则化解释
  - Shrinkage，对应参数里面的learning_rate，及我们每次对每棵树的拟合进行缩放。实践中一般把此参数设置很小，比如0.01之类的，然后配合early stopping来使用。
  - Subsample，子采样，取值为 (0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做XGB的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但会增加样本拟合的偏差，因此取值不能太低。推荐在 [0.5, 0.8]之间。注意这里的不放回是指当前这棵树，每棵树之间毫无关系。
  - Early Stopping，对应参数里面的tol，机器学习迭代式训练模型中很常见的防止过拟合技巧，具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。在sklearn的GBDT中可以设置参数n_iter_no_change和tol实现early stopping。
  - 与GBDT最大的不同是XGB在目标函数里面就加入了正则化，正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。这样每次增加分裂都能有一定的penalty，有效地控制了树的复杂度。
  - 列抽样也能有效的降低每次树拟合的复杂度，达到降低variance的效果。
  - 其它正则化的方法就是对每次拟合的单一CART树进行限制，此处参考decision tree里面的note详细介绍了单一树的正则化。
- 其它要点 
  - 子采样，借用bootstrap的思想，每一轮训练时只使用一部分样本，不同点是这里的采样是无放回抽样，这个方法被称为Stochastic Gradient Boosting。对于单棵树来说，只使用一部分样本拟合会增加单棵树的偏差和方差，然而subsampling会使树与树之间的相关性减少，从而降低模型的整体方差，很多时候会提高准确性。subsampling的另一个好处是因为只使用一部分样本进行训练，所以能显著降低计算开销。
  - 大部分单一树的要点可以参考decision tree里面的note。
  - 总的来说，对于XGB build的时间复杂度一般是O(n * log(n) * depth * n_estimators)，predict的时间复杂度是O(log(n) * n_estimators)，n是样本个数，也就是每科单一树的时间乘以总共几棵树，这里和GBDT最大的不一样是支持feature直接并行运算找最佳分裂特征。
  - 除了以上最大的不同外，同时还有近似算法，稀疏感知，工程上用列块并行提前存储排序后的特征值和一阶导数二阶导数信息，缓存访问以及核外块计算利用硬盘加独立的线程加速读取数据等工程上的优化。
  - XGB对比RF可参考GBDT里面的note详细介绍了各方面的对比。
  - XGB使用二阶导数信息，因为二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。
  - 总的来说，XGB是大数据工程实现并提高精度和降低方差能力版本的GBDT。
### Engineer Work
- 一般来说直接调用sklearn中的XGB接口来实现。实际生产中普通版本目前来说在(440, 10)的数据集上每棵树拟合需要6s左右在8cores/16Memory上。
- 如需要再1M以上的数据跑production，目前本人测试过的情况是在GCP上1M的数据还是很容易实现进pipeline的。
- 如果需要跑更大的数据，推荐参考spark版本带GPU的XGB。[参考link](https://xgboost.readthedocs.io/en/stable/tutorials/spark_estimator.html#xgboost-pyspark-estimator)


### 面试问题总结
1. 请问（决策树、Random Forest）GBDT和XGBoost的区别是什么？
   - 集成学习的集成对象是学习器。Bagging和Boosting属于集成学习的两类方法。Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器，迭代集成(平滑加权)。
   - 决策树属于最常用地学习器，其学习过程是从根建立树，也就是如何决策叶子节点分裂。ID3/C4.5决策树用信息熵计算最优分裂，CART决策树用基尼指数计算最优分裂，xgboost决策树使用二阶泰勒展开系数计算最优分裂。
   - xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：
     - 基分类器：
       - XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。
     - 导数信息：
       - XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。
     - 正则项：
       - XGBoost的目标函数加了正则项，相当于预剪枝，防止模型过度复杂，使得学习出来的模型更加不容易过拟合。
     - 列抽样：
       - XGBoost支持列采样，与随机森林类似，用于防止过拟合。
     - 缺失值处理：
       - 对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。
     - 计算分裂节点方式：
       - 节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过二阶泰勒展开后加上正则化优化推导后的系数。
     - 并行化：
       - 不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。
       - 工程上还有实现了各种加速和memory save。
2. xgboost要用泰勒展开，优势在哪里？
   - xgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降得更快更准，因为二阶导数是梯度的变化率，控制梯度的变化方向和速度。
   - 使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂优化计算，因为xgb的不纯度和叶子结点的权重公式推导和具体是什么损失函数无关，本质上也就把损失函数的选取和模型算法优化/参数选择分开了。这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，可以用于分类，也可以用于回归。
3. xgboost如何寻找最优特征？是又放回还是无放回的呢？
   - xgboost在每棵树的训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据，从而记忆了每个特征对在模型训练时的重要性，从根到叶子中间节点涉及某特征的次数作为该特征重要性排序之一。
   - xgboost属于boosting集成学习方法，样本是不放回的，因而每轮计算样本不重复。另一方面, xgboost支持子采样，也就是每轮计算可以不使用全部样本，以减少过拟合。进一步地，xgboost 还有列采样，每轮计算按百分比随机采样一部分特征，既提高计算速度又减少过拟合。
   - 详细讲解放不放回的采样：
     - bagging：有放回的抽样
       - 对于随机森林来说，建立很多棵树，每棵树的样本数量(可能)全量样本的个数，但是每棵树又得不一样(好而不同)，数据最好也是不同的，但是一共就那么多数据，怎么能数据不同？那么每一次都是bootstrap到n个样本。所以bagging是放回抽样。
     - boosting： 无放回的抽样
       - 每一次建立的树都是使用 alpha*n 个样本建立的，每一次都是随机的从全量样本中抽这么多的样本建立树。而且 alpha 一般都是0.5~0.8。这里的alpha即为上面参数的subsample。对于每棵树，数据是不重复的。但是对于树与树之间的数据是有很大的重复的。
       - 对于每次建树，都会抽一个子集。抽这个子集的过程，对于bagging是自助采样，得到的训练集中有的数据会出现多次，自助采样是放回的。对于boosting是直接做subsample*N的随机抽样，得到的训练集是没有重复样本的。放回和不放回都是针对单次构建训练集来说。每次构建训练集应视为独立的事件，所以不放回的抽样每次仍然是从全训练集做的抽样。
4. 为什么boosting不能采用放回抽样？ 
   - 如果每棵树的构建时候采用放回抽取会造成样本重复，相当于将此样本的重要性加倍，对于boosting来说，由于模型是越来越强，样本的重要性加倍很容易造成过拟合，而且不放回的抽样本质上就是防止过拟合的方式之一。
5. XGB如何处理缺失值？
   - 训练阶段：
     - 采用的稀疏感知算法可以自动学习出它的分裂方向。在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销。在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向。
   - 预测阶段：
     - 如果训练数据没有缺失值，但是预测数据含有缺失值，那么每次分裂默认被分类到右子树。
   - 预测阶段风险：
     - 即我们假设了训练数据和预测数据的分布相同，并且缺失值的分布也是相同的，但是理论上这样是没有问题，因为我们就是要保证训练数据分布最大趋向实际预测数据分布。
6. XGB为什么怎么快，工程上用了那些加速方式？
   - 分块并行：
     - 训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点
   - 候选分位点：
     - 每个特征采用常数个分位点作为候选分割点
   - CPU cache 命中优化：
     - 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。
   - Block 处理优化：
     - Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐
7. XGBoost中叶子结点的权重如何计算出来?
   - 目标函数公式，参考链接1里面有详细推导，![图片](/pics/xgb_object_function.webp)
   - 当目标函数达到最小值Obj时，每个叶子结点的权重为wj。![图片](/pics/xgb_weight_function.webp)
8. XGBoost如何处理不平衡数据?
   - 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10。源码会自动在计算少样本落在叶子结点的权重的时候，乘以设置好的参数。
   - 第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。
9. 比较LR和GBDT，说说什么情景下GBDT/XGB不如LR？
   - 区别： 
     - LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程。
     - GBDT，XGB是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合。
   - 什么情况下GBDT/XGB效果不如LR：
     - 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：
       - 例子： 
       - 假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。
       - 那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况：y = W1*f1 + Wi*fi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。
       - 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：**带正则化的线性模型比较不容易对稀疏特征过拟合**。
10. XGBoost如何评价特征的重要性？
    - weight：该特征在所有树中被用作分割样本的特征的总次数。
    - gain：该特征在其出现过的所有树中产生的平均增益。
    - cover：该特征在其出现过的所有树中的平均覆盖范围。覆盖范围指的是一个特征用作分割点后，其影响的样本数量，既有多少样本经过该特征分割到两个子节点。
11. XGBoost参数调优的一般步骤
    - 一般需要优化的参数：
      - max_depth = 5 
      - min_child_weight = 1 
      - gamma = 0 
      - subsample
      - col_sample_by* = 0.8 
      - scale_pos_weight = 1
      - learning_rate = 0.3
      - estimator = 100
    - learning rate 和 estimator 的数量，learning rate可以先用0.1，用cv来寻找最优的estimators。
    - max_depth 和 min_child_weight，调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。
      - max_depth，每棵子树的最大深度，check from range(3,10,2)。 
      - min_child_weight，子节点的权重阈值，如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。check from range(1,6,2)。
    - gamma 也称作最小划分损失min_split_loss，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。如果大于该阈值，则该叶子节点值得继续划分，如果小于该阈值，则该叶子节点不值得继续划分。
    - subsample，col_sample_by*，subsample是对训练的采样比例，colsample_bytree是对特征的采样比例，both check from 0.6 to 0.9。
    - 正则化参数 alpha 是L1正则化系数，try 1e-5，1e-2，0.1，1，100，lambda 是L2正则化系数。
    - 最后都不行的话，可以再试一试降低学习率，降低学习率的同时增加树的数量，通常最后设置学习率为0.01 ~ 0.1。
12. XGBoost模型如果过拟合了怎么解决？
    - 用于直接控制模型的复杂度。包括max_depth，min_child_weight，gamma 等参数。
    - 用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample，col_sample_by*。
    - 用于控制整个XGB系列树的长度，比如learning rate 和 estimator。再配合early stopping。

