
### 基本原理
参考链接：
- [中文简单直接的原理讲解](https://zhuanlan.zhihu.com/p/97753849)
- [中文详细介绍和理论论文研读](https://blog.csdn.net/weixin_44750583/article/details/99431770)
- [英文原理讲解及例子](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)
- [sklearn中相关应用文档以及详细解释](https://scikit-learn.org/stable/modules/ensemble.html)
### 优点
- 每一个树都是CART树，容易理解和解释结果和算法过程。可以可视化结果解释。
- 对比简单的单一模型，集成学习模型可以防止过拟合。RF的最大好处在于，行抽样和列抽样的引入让模型具有抗过拟合和抗噪声的特性。 
- 对于树类型的模型，不需要对数据做预处理，满足各种类型的数据无论是离散和连续，不需要归一化，不需要标准化数据，可以将缺失值单独作为一类处理，也不需要满足什么数据前提假设。能灵活地处理各种类型的数据。
- 集成学习模型的非线性变换比较多在构建一系列的树的时候，表达能力强，而且不需要做复杂的特征工程和特征变换。
- 可解释性强，可以自动做特征重要性排序。因此常作为发现特征组合的有效思路。
- 可以解决几乎所有单一树模型的缺点。又同时集成了所有单一树模型的优点。
- 可以处理多维度输出的分类问题，及多分类问题。
- 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
- 在相对较少的调参时间下，预测的准确度较高。
- 不同树的生成是并行的，训练速度优于GBDT算法，特别是当树的个数比较多的时候。
- RF的训练效率会高于GBDT，因为在单个决策树的构建中，GBDT使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。
- RF相比较GBDT对异常值不敏感，这也是为什么RF更偏向于降低variance。
### 缺点
- 对比GBDT，RF的起始性能较差，特别当只有一个基学习器时，随机森林通常会收敛到更低的泛化误差。RF一般降低variance，而GBDT一般降低bias。
- 对于高维稀疏特征，如果feature个数太多，并且很稀疏，每一棵回归树都要耗费大量时间，这一点上大大不如SVM。
- 工程上虽然可以对每棵树进行parallel训练加速训练时间，但是会消耗memory。
### 知识点提炼
- 参数解释
  - n_estimators，构建多少个并行的DT，一般来说RF每棵树都很简单，所以总共多少棵树一般选择比较大的值比如200，然后配合early stopping防止过拟合。
  - max_depth，单个决策树的最大深度，和减枝有关，如果不设置，则一直build树到底。和GBDT不一样的是，这个参数对于RF来说一般不会限制，因为我们需要每棵树在训练样本的子集上达到最好，至于拟合问题，最后会通过集合所有树结果来达到降低过拟合。
  - min_samples_split，单个决策树分裂后子树最小需要多少个样本才可以去分裂一个树节点。
  - min_samples_leaf，单个决策树节点构建叶子结点的时候，叶子结点最少需要多少个样本来构建。如果分裂后低于这个参数则意味着不再分裂此节点。此参数可能会影响回归任务的平滑效果。
  - min_impurity_decrease，单个决策树节点最小需要降低多少impurity，如果分裂后低于此参数则意味着不再分裂此节点。此参数可以达到early stopping的效果，防止过拟合太多的单一决策树。
  - max_features，每次构建训练单一树用的子集的时候可以最多选择的特征个数，如果不给定的话，分类问题默认使用所有特征的个数开方后的结果，回归问题默认使用所有特征。
- 损失函数解释
  - 回归问题，CART回归树，平方误差最小化。及最小化分裂后区域target值的variance。
  - 分类问题，CART分类树，基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。
- 正则化解释
  - bootstrap，行抽样，取值为整个样本个数，但是随机森林使用的是放回抽样，也就是说有的样本会被选中多次，如果抽样次数足够多的时候，大概每一轮抽样中会有36.8%的数据不被抽到。
  - max_features，列抽样，随机森林会随机选取m个特征 (m < M) 训练用于每一棵CART树的生成。当m越小时，模型的抗干扰性和抗过拟合性越强，但是模型的准确率会下降，因此在实际建模过程中，常需要用交叉验证等方式选择合适的m值。
  - 其它正则化的方法就是对每次拟合的单一CART树进行限制，此处参考decision tree里面的note详细介绍了单一树的正则化。
- 其它要点 
  - 大部分单一树的要点可以参考decision tree里面的note。
  - 总的来说，对于RF build的时间复杂度一般是O(m * n * log(n))，predict的时间复杂度是O(log(n))，m是feature个数，n是样本个数，也就是构建每颗单一树的时间。
  - GBDT对比RF：
    - 1）集成的方式：随机森林属于Bagging思想，而GBDT是Boosting思想。
    - 2）偏差-方差权衡：RF不断地降低模型的方差，而GBDT不断地降低模型的偏差。
    - 3）训练样本方式：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本，此处现在可以通过子采样来同样达到部分样本训练，不过是无放回的抽样。
    - 4）并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)。
    - 5）最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合。
    - 6）数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感。
    - 7）泛化能力：RF不易过拟合，而GBDT容易过拟合。
  - 所有树的模型都有一个共性就是可以得到特征重要性 (feature importance)，具体计算方式为，单一树此特征作为分裂点得到的impurity grain * 此特征被用作分裂点的次数 * 总共多少棵树用了此特征，再对所有特征的结果做归一化，得到最后每个特征的重要性。可用于特征选择，也可用于特征解释。
  - 随机森林虽然泛化能力很强，但是还是会过拟合，当树的个数越来越多的时候，泛化能力会趋于一个极限值，也就是会收敛到一个极限误差值，简单点说就是随机森林的泛化误差界与单个决策树的分类强度s成负相关，与决策树之间的相关性ρ成正相关，分类强度s越大且相关性ρ越小，泛化误差界越小，刚好随机森林中的随机性可以保证ρ越小，如果每棵树的s越大的话，泛化误差会收敛到一个很小的margin，这个margin越小越好，就是泛化误差越小。[参考讨论链接](https://www.zhihu.com/question/30295075)
### Engineer Work
- 一般来说直接调用sklearn中的RF包来实现。实际生产中普通版本目前来说在(150, 4)的数据集上每棵树拟合需要0.7s左右在8cores/16Memory上。
- 如需要再1M以上的大数据跑production，随机森林是个可行的选择。推荐参考spark版本的regression和classification。[参考link](https://spark.apache.org/docs/latest/ml-classification-regression.html)


### 面试问题总结
1. 随机森林需要交叉验证吗？
   - 直接答案是不需要，因为随机森林在训练过程中每次都会有一部分数据没有被用于训练，基于bootstrap的抽样，这部分数据我们一般叫OOB数据，我们可以用这部分数据作为验证集数据，而不需要额外划分数据作为验证集。[参考链接](https://blog.csdn.net/weixin_44750583/article/details/99431770)
   - 具体操作：
     - 对于数据集中每一个样本，收集出没有用此样本训练的决策树，组合成一个子随机森林
     - 对这个子随机森林进行误差计算，得到当前此样本的验证结果
     - 重复上述所有过程对每一个样本
     - 对所有样本最后的结果取平均值，作为整个随机森林的交叉验证结果