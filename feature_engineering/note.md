## feature preprocessing (特征预处理)

### feature missing (特征缺失)
#### 参考链接：
- [中文数据预处理总结](https://www.zhihu.com/tardis/zm/art/100442371)
- [中文数据缺失方法总结](https://blog.csdn.net/Li_yi_chao/article/details/99303331)
- [中文数据预处理方法带代码例子](https://zhuanlan.zhihu.com/p/87020955)
- [英文数据缺失方法总结](https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e)
#### 知识点提炼:
1. 直接使用含有缺失值的特征，此方法需要model本身可以handle缺失数据，比如各种XGB，LGB。
2. 删除含有缺失值的特征：若变量的缺失率较高（大于80%），覆盖率较低，且重要性较低，可以直接将变量删除。
3. 数据插补(imputation)：
   - 均值插补。数据的属性分为定距型和非定距型。如果缺失值是定距型的，就以该属性存在值的平均值来插补缺失的值；如果缺失值是非定距型的，就根据统计学中的众数原理，用该属性的众数(即出现频率最高的值)来补齐缺失的值。定距型数据--数据的中间级，用数字表示个体在某个有序状态中所处的位置，不能做四则运算。例如，“受教育程度”，文盲半文盲=1，小学=2，初中=3，高中=4，大学=5，硕士研究生=6，博士及其以上=7。
   - 利用同类均值插补。同均值插补的方法都属于单值插补，不同的是，它用层次聚类模型预测缺失变量的类型，再以该类型的均值插补。假设X=(X1,X2…Xp)为信息完全的变量，Y为存在缺失值的变量，那么首先对X或其子集行聚类，然后按缺失个案所属类来插补不同类的均值。比如KNN。
   - 极大似然估计（MLE）。在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计。这种方法也被称为忽略缺失值的极大似然估计，对于极大似然的参数估计实际中常采用的计算方法是期望值最大化(Expectation Maximization，EM）。该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。
   - 随机插补填充，用已知的特征值随机插补，此方法可以配合进行特征选择，当进行k-fold交叉验证的时候如果每次特征的随机插补都不影响特征的重要性的话，那说明其实我们可以不用这个有缺失的特征。

### feature distribution (特征分布)
#### 参考链接：
- [中文数据预处理总结1](https://www.zhihu.com/tardis/zm/art/100442371)
- [中文数据预处理总结2](https://zhuanlan.zhihu.com/p/364402999)
- [中文异常值检查方法总结](https://zhuanlan.zhihu.com/p/524732373)
- [中文不平衡问题方法总结](https://zhuanlan.zhihu.com/p/364402999)
- [中文不平衡问题总结](https://zhuanlan.zhihu.com/p/556158050)
#### 知识点提炼:
1. 对于numerical数据直接做plot，对于categorical数据做histogram。初步了解每个特征的分布，比如是否接近normal distribution或者有明显额层次分布之类的，看看是否满足model的assumption，方便后续对model进行选择。
2. 计算numerical数据之间的correlation metric，更加直观的了解特征之间是否有强关联性，对后续做特征选择和model选择有一定的帮助。
3. 发现数据是否存在异常值，对于异常值我们可以采用一下方式检测并处理：
   - 检测方法：
     - 3sigma，对于numerical数据大于或者小于 3倍的sigma的数据都是异常值。
     - box-plot，四分位间距对定义离群点非常重要。它是第三个四分位数和第一个四分位数的差 (IQR = Q3 -Q1)。在这种情况下，离群点被定义为低于箱形图下触须（或 Q1 − 1.5x IQR）或高于箱形图上触须（或 Q3 + 1.5x IQR）的观测值。
     - ML的检测方法，监督学习构建异常值的label，无监督学习采用基于密度的聚类方法，比如DBSCAN。不过此处我们重点在于数据的预处理，所以构建model去detect异常值不是重点。
   - 解决方法：
     - 基于数据的角度，根据异常点的数量和影响，考虑是否将该条记录删除。但是会有信息损失
     - 基于数据的角度，对于numerical数据我们可以做一些transform，比如log-scale 对数变换后消除了异常值，则此方法生效，且不损失信息。
     - 基于数据的角度，平均值或中位数替代异常点，简单高效，信息的损失较少。
     - 基于model的角度，在训练树模型时进行选择对异常值不明感的model，比如树模型对离群点的鲁棒性较高，无信息损失，不影响模型训练效果。
     - 基于损失函数的角度，在训练的时候选择对异常值损失函数不明感的函数，比如回归问题选择MAE而不是MSE，比如SVM里面选择hinge loss。具体参考损失函数部分。
4. 发现数据的label是否存在不平衡的问题，对于分类问题，计算一下每个class的个数，一般超过1：10视为不平衡。对于不平衡的数据我们可以采用一下方式处理：
   - 欠采样(under-sampling)：
     - 随机欠采样，对多数类样本中随机选择少量样本与少数类数据合并达到balance，即舍弃一些多数类数据，使其与少数类数据平衡，这样做会丢失过多的数据，最终的模型也只能学习到总体模式的一部分。
     - Easy Ensemble欠采样，从多数类中有放回的随机采样n次，每次选取与少数类相近的样本数。将采样样本和少数类样本构造n个样本集S1，S2，Sn 训练出n个模型，进行Ensemble。有放回的随机采样，训练时往往会存在重复样本。更常用的方式是：从多数类中无放回的随机采样n次。预测阶段也需要预测n次再进行ensemble预测结果。
     - 迭代预分类欠采样，每一轮选出多样本的一部分数据和少样本一起训练，训练结束后对没有选择的多样本进行预测，预测label错误的样本我们加入前一轮的数据集中继续训练model，直到训练的第N轮分类器可以全部识别多样本候选集。相比较上面两种方法，此方法最大限度的利用负样本中差异性较大的负样本，从而在控制正负样本比例的基础上采样出了最有代表意义的负样本。但是同时需要注意的点：将预测错误的样本放入下一轮训练时，需要判断一下这些样本是否是异常样本，否则将异常样本持续放入模型训练，只会影响模型的正确学习。
   - 过采样(over-sampling)：
     - 随机过采样，从少样本中有放回的重复采样，直到获取了满足样本比例的少样本，也就是说有些少样本可能会被重复采样到。缺点是模型训练复杂度加大，容易造成过拟合，因为生成了很多的重复样本。
     - SMOTO，对每个少数类样本a，从它的最近邻（欧式距离）中随机选择一个样本b(少数类中的一个样本)，然后在a和b之间的连线上随机选择一点作为新合成的少数类样本。可以防止随机过采样容易过拟合的问题。缺点是由于对每个少数类样本都生成新样本，容易发生样本重叠问题，并没有考虑少数样本周围多数类样本的分布情况。比如多样本分布在少样本之间的连线上或者周围，新生成的点离多数类样本很近，此时很容易就导致它们有可能被划分为多数类样本。
   - 损失函数层面：
     - 加权损失函数，比如对于CELoss，负样本为多样本个数为a，正样本为少样本个数为b，此时正样本的 loss * b/(a + b)，负样本的 loss * a/(a + b)，这样相当于控制模型更加关注少样本对损失函数的影响。通过这种基于类别的加权的方式可以从不同类别的样本数量角度来控制Loss值，从而一定程度上解决了样本不均衡的问题。
   - 其它方法：
     - 选择树类型的ensemble model，会不那么明感对于不平衡问题。
     - 调整阈值修改正负样本比例：通常情况会将大于阈值0.5的样本预测为正样本。这时候我们可以通过调节阈值来调整正负样本的比例，比如设置阈值0.3，大于0.3的样本都判定为正样本，相当于增加了正样本的比例。
     - 利用半监督或者自监督学习。
     - 使用新评价指标，具体参考损失metric部分。

### feature transformer (特征转换)
#### 参考链接：
- [中文数据预处理总结1](https://www.zhihu.com/tardis/zm/art/100442371)
- [中文特征编码总结1](https://zhuanlan.zhihu.com/p/441284257)
- [中文特征编码总结2](https://zhuanlan.zhihu.com/p/67475635)
- [英文总结特征缩放](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)
- [中文总结特征缩放1](https://zhuanlan.zhihu.com/p/95726841)
- [中文总结特征缩放2](https://www.jianshu.com/p/95a8f035c86c)
#### 知识点提炼:
1. 特征编码，有时候对于categorical特征我们的特征分布值比较多或者numerical特征分布很广平，我们需要做一些特征转换方便后续model使用。
   - 分箱特征二元化/多元化，特征二元化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。多元化同理在类别型的属性上选择某几个值归为一个新的属性值。或者在数值型的属性上划分多个区间。
   - 独热编码（One-HotEncoding）独热编码采用N位状态寄存器来对N个可能的取值进行编码，每个状态都由独立的寄存器来表示，并且在任意时刻只有其中一位有效。独热编码的优点：能够处理非数值属性，在一定程度上扩充了特征，编码后的属性是稀疏的，存在大量的零元分量。
   - 对于多属性的类别型特征，我们可以转化成数值型特征，比如采用频率编码，TF-IDF编码，或者embedding，详细参考上面的链接。
   - 对于会连续变动的数值型特征，我们可以转化成比例，这样可以保证无论数值如何改动做预测的时候特征的含义都不会造成影响。
2. 特征缩放，一般基于我们的model选择，我们需要对数值型特征做一些标准化。比如对于基于gradient descend，distance的model或者PCA之类的需要计算cov matrix的model，我们需要标准化。对于树类型的model或者discriminate model（LDA，NB）我们并不需要。另外的原因包括，需要消除样本不同属性具有不同量级时的影响：数量级的差异将导致量级较大的属性占据主导地位，数量级的差异将导致迭代收敛速度减慢，依赖于样本距离的算法对于数据的数量级非常敏感。归一化后求优过程范围变小，寻优过程变得平缓，更容易正确收敛到最优解。
   - 标准化（Standardization）：减去均值除于方差，也就是将数据变为均值为0方差为1的正态分布（注意，这里指的是数据的分布，数值不一定在0-1区间，区分归一化）。标准化在建模过程中，使得各个特征维度无差别对待，消除一些数值尺度差异带来的重要偏见。经过上述处理的数据，能加快模型训练速度，促进收敛；标准化有一个很重要的性质：线性变换不会改变原始数据的数值排序。标准化处理本质上是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是标准化的前提。
   - 归一化（Normalization）：把数据大小限定在特定的范围，比如一般都是 [0,1] 或 [-1,1]，转化成[0,1]区间的同时也叫 Min-Max scaling。归一化主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。其次把有量纲表达式变成无量纲表达式意思是去除单位的意义，便于不同单位或量级的指标能够进行比较和加权。
   - 一般来说优先使用标准化。对于输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。如果数据较为稳定，不存在极端的最大最小值，也可以先用用归一化。如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。


## feature selection (特征选择)

### feature reduction (特征降维)
#### 参考链接：
- [中文数据预处理总结1](https://www.zhihu.com/tardis/zm/art/100442371)
- [英文总结维度灾难](https://www.analyticsvidhya.com/blog/2021/04/the-curse-of-dimensionality-in-machine-learning/)
- [中文总结特征降维及提取方法](https://zhuanlan.zhihu.com/p/43225794)
- [中文总结降维及例子](https://zhuanlan.zhihu.com/p/82417864)
#### 知识点提炼:
1. 为什么要降维：
   - 数据维度越高，则需要更大的数据量来进行数据表示，数据量指数级增大。
   - 特征维度越高，学得的模型越复杂，更容易出现过拟合的问题。
   - 高维情况下，容易出现数据样本稀疏问题。
   - 高维空间会给距离计算带来很大困难。
   - 高位空间的距离变的没有意义，对于高纬度的数据直接做基于距离上的模型是毫无意义的。
   - 减少冗余特征或噪声数据。
   - 计算量更小，训练速度更快。
   - 数据存储所占空间（内存）减少。
   - 若将维度降为二维或三维，则方便可视化。
   - 把原本稀疏的全局空间分布边的更加紧凑，利于下游任务拟合model。
2. 常见地降维方法：
   - 线性降维：
     - PCA，无监督学习，PCA着重描述特征，PCA变换矩阵是正交的。具体细节参考PCA章节。
     - LDA，有监督学习，LDA着重抓住其判别特征，LDA一般不是正交的。
   - 非线性降维：
     - T-sne，主要用于可视化了解大概数据分布的整体形状为下游任务准备。具体细节参考T-sne章节。
     - UMAP，同T-sne原理，不过更加适用于提取global信息和可以适用于降到任意维度相比较与T-sne。
     - Autoencoder，采用神经网络的方式，两层编码，两层解码，decode 的最后一层节点个数要跟输入维度一致，原始向量经过编码器和解码器，得到最终输出，将输出与输入比较就可以计算误差（比如取 MSE）。当模型训练完毕，去掉所有 decode 层，就是一个降维的模型了，也就是encode的最后一层是降维结果。

### feature extraction (特征提取)
#### 参考链接：
- [中文总结特征降维及提取方法](https://zhuanlan.zhihu.com/p/43225794)
- [中文特征工程中的特征选择讨论](https://www.zhihu.com/question/28641663)
#### 知识点提炼:
1. 人工选择重要特征，需要比较多的domain knowledge或者之前做实验的experience。
2. 缺失值比率（Missing Value Ratio），如果缺失值较高并且此特征并没有太多作用通过domain knowledge，那么我们可以直接删除此特征，阈值可以设置为10%。
3. 低方差过滤（Low Variance Filter），如果特征的数值基本一致，也就是它的方差非常低，和上一种方法的思路一致，我们通常认为低方差变量携带的信息量也很少，所以可以把它直接删除。实践中，就是先计算所有变量的方差大小，然后删去其中最小的几个。需要注意的一点是：方差与数据范围有相关性，因此在采用该方法前需要对数据做归一化处理。一般是放到同一个range区间内，而不是标准化不然方差将变的都一样。
4. 高相关过滤（High Correlation filter），如果两个变量之间是高度相关的，这意味着它们具有相似的趋势并且可能携带类似的信息。同理，这类变量的存在会降低某些模型的性能（例如线性和逻辑回归模型）。为了解决这个问题，我们可以计算独立数值变量之间的相关性。如果相关系数超过某个阈值，就删除其中一个变量。一般来说如果两个特征相关性大于0.5就要考虑是否是否要删除一个变量。
5. 反向特征消除（Backward Feature Elimination），一般来说我们用线性model来执行这个过程，因为线性model更快，以下是反向特征消除的主要步骤： 
   - 先获取数据集中的全部n个变量，然后用它们训练一个模型。
   - 计算模型的性能。
   - 在删除每个变量（n次）后计算模型的性能，即我们每次都去掉一个变量，用剩余的n-1个变量训练模型。
   - 确定对模型性能影响最小的变量，把它删除。
   - 重复此过程，直到不再能删除任何变量。
6. 前向特征选择（Forward Feature Selection），同反向的原理，只是反过来找到能改善模型性能的最佳特征，而不是删除弱影响特征。不过前向特征选择和反向特征消除耗时较久，计算成本也都很高，所以只适用于输入变量较少的数据集。以下是前向特征选择主要步骤：
   - 选择一个特征，用每个特征训练模型n次，得到n个模型。
   - 选择模型性能最佳的变量作为初始变量。
   - 每次添加一个变量继续训练，重复上一过程，最后保留性能提升最大的变量。
   - 一直添加，一直筛选，直到模型性能不再有明显提高。
7. 嵌入式特征提取，这里主要指靠model进行的特征选择model拟合阶段进行。
   - 基于惩罚项的特征选择法，使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。比如logistics regression中使用L1或者L2或者都用进行特征限制和选择。
   - 树类型的model，都可以自己计算每个feature的importance，具体怎么计算参考model章节，我们可以根据importance的排序从上到下选择我们想要的特征。

