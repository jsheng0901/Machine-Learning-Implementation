## feature preprocessing (特征预处理)

### feature missing (特征缺失)
#### 参考链接：
- [中文数据预处理总结](https://www.zhihu.com/tardis/zm/art/100442371)
- [中文数据缺失方法总结](https://blog.csdn.net/Li_yi_chao/article/details/99303331)
- [中文数据预处理方法带代码例子](https://zhuanlan.zhihu.com/p/87020955)
- [英文数据缺失方法总结](https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e)
#### 知识点提炼:
1. 直接使用含有缺失值的特征，此方法需要model本身可以handle缺失数据，比如各种XGB，LGB。
2. 删除含有缺失值的特征：若变量的缺失率较高（大于80%），覆盖率较低，且重要性较低，可以直接将变量删除。
3. 数据插补(imputation)：
   - 均值插补。数据的属性分为定距型和非定距型。如果缺失值是定距型的，就以该属性存在值的平均值来插补缺失的值；如果缺失值是非定距型的，就根据统计学中的众数原理，用该属性的众数(即出现频率最高的值)来补齐缺失的值。定距型数据--数据的中间级，用数字表示个体在某个有序状态中所处的位置，不能做四则运算。例如，“受教育程度”，文盲半文盲=1，小学=2，初中=3，高中=4，大学=5，硕士研究生=6，博士及其以上=7。
   - 利用同类均值插补。同均值插补的方法都属于单值插补，不同的是，它用层次聚类模型预测缺失变量的类型，再以该类型的均值插补。假设X=(X1,X2…Xp)为信息完全的变量，Y为存在缺失值的变量，那么首先对X或其子集行聚类或者分类如果有label的情况下，然后按缺失个案所属类来插补不同类的均值。比如聚类对应用KMeans，分类任务用KNN。
   - 利用模型进行预测，比如将缺失值作为target进行预测，先用没有缺失值的样本进行train一个模型，然后再对有缺失值的样本进行predict，缺点就是可能其它特征和此缺失值特征毫无关系，则预测的结果也毫无意义，不过也侧面说明缺失值特征可能毫无意义，可以不需要放进数据集中。
   - 极大似然估计（MLE）。在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计。这种方法也被称为忽略缺失值的极大似然估计，对于极大似然的参数估计实际中常采用的计算方法是期望值最大化(Expectation Maximization，EM）。该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。
   - 随机插补填充，用已知的特征值随机插补，此方法可以配合进行特征选择，当进行k-fold交叉验证的时候如果每次特征的随机插补都不影响特征的重要性的话，那说明其实我们可以不用这个有缺失的特征。
   - 单独归为一类，如果特征缺失值比较多但是有不是特别多，并且从domain knowledge来说可能是有用特征，比如30%，可以尝试单独给缺失值归为一类特征值，这些缺失值说不定会包含一些趋势信息。

### feature distribution (特征分布)
#### 参考链接：
- [中文数据预处理总结1](https://www.zhihu.com/tardis/zm/art/100442371)
- [中文数据预处理总结2](https://zhuanlan.zhihu.com/p/364402999)
- [中文异常值检查方法总结](https://zhuanlan.zhihu.com/p/524732373)
- [中文不平衡问题方法总结](https://zhuanlan.zhihu.com/p/364402999)
- [中文不平衡问题总结](https://zhuanlan.zhihu.com/p/556158050)
#### 知识点提炼:
1. 对于numerical数据直接做plot，对于categorical数据做histogram。初步了解每个特征的分布，比如是否接近normal distribution或者有明显额层次分布之类的，看看是否满足model的assumption，方便后续对model进行选择。
2. 计算numerical数据之间的correlation metric，更加直观的了解特征之间是否有强关联性，对后续做特征选择和model选择有一定的帮助。
3. 发现数据是否存在异常值，对于异常值我们可以采用一下方式检测并处理：
   - 检测方法：
     - 3sigma，对于numerical数据大于或者小于 3倍的sigma的数据都是异常值。
     - box-plot，四分位间距对定义离群点非常重要。它是第三个四分位数和第一个四分位数的差 (IQR = Q3 -Q1)。在这种情况下，离群点被定义为低于箱形图下触须（或 Q1 − 1.5x IQR）或高于箱形图上触须（或 Q3 + 1.5x IQR）的观测值。
     - ML的检测方法，监督学习构建异常值的label，无监督学习采用基于密度的聚类方法，比如DBSCAN。不过此处我们重点在于数据的预处理，所以构建model去detect异常值不是重点。
   - 解决方法：
     - 基于数据的角度
       - 根据异常点的数量和影响，考虑是否将该条记录删除。但是会有信息损失 
       - 对于numerical数据我们可以做一些transform，比如log-scale 对数变换后消除了异常值，则此方法生效，且不损失信息。
       - 平均值或中位数替代异常点，简单高效，信息的损失较少。
     - 基于model的角度
       - 在训练树模型时进行选择对异常值不明感的model，比如树模型对离群点的鲁棒性较高，无信息损失，不影响模型训练效果。
     - 基于损失函数的角度
       - 在训练的时候选择对异常值损失函数不明感的函数，比如回归问题选择MAE而不是MSE，比如SVM里面选择hinge loss。具体参考损失函数部分。
4. 发现数据的label是否存在不平衡的问题，对于分类问题，计算一下每个class的个数，一般超过1：10视为不平衡。对于不平衡的数据我们可以采用一下方式处理：
   - 欠采样(under-sampling)：
     - 随机欠采样，对多数类样本中随机选择少量样本与少数类数据合并达到balance，即舍弃一些多数类数据，使其与少数类数据平衡，这样做会丢失过多的数据，最终的模型也只能学习到总体模式的一部分。
     - Easy Ensemble欠采样，从多数类中有放回的随机采样n次，每次选取与少数类相近的样本数。将采样样本和少数类样本构造n个样本集S1，S2，Sn 训练出n个模型，进行Ensemble。有放回的随机采样，训练时往往会存在重复样本。更常用的方式是：从多数类中无放回的随机采样n次。预测阶段也需要预测n次再进行ensemble预测结果。
     - 迭代预分类欠采样，每一轮选出多样本的一部分数据和少样本一起训练，训练结束后对没有选择的多样本进行预测，预测label错误的样本我们加入前一轮的数据集中继续训练model，直到训练的第N轮分类器可以全部识别多样本候选集。相比较上面两种方法，此方法最大限度的利用负样本中差异性较大的负样本，从而在控制正负样本比例的基础上采样出了最有代表意义的负样本。但是同时需要注意的点：将预测错误的样本放入下一轮训练时，需要判断一下这些样本是否是异常样本，否则将异常样本持续放入模型训练，只会影响模型的正确学习。
   - 过采样(over-sampling)：
     - 随机过采样，从少样本中有放回的重复采样，直到获取了满足样本比例的少样本，也就是说有些少样本可能会被重复采样到。缺点是模型训练复杂度加大，容易造成过拟合，因为生成了很多的重复样本。
     - SMOTO，对每个少数类样本a，从它的最近邻（欧式距离）中随机选择一个样本b(少数类中的一个样本)，然后在a和b之间的连线上随机选择一点作为新合成的少数类样本。可以防止随机过采样容易过拟合的问题。缺点是由于对每个少数类样本都生成新样本，容易发生样本重叠问题，并没有考虑少数样本周围多数类样本的分布情况。比如多样本分布在少样本之间的连线上或者周围，新生成的点离多数类样本很近，此时很容易就导致它们有可能被划分为多数类样本。
   - 损失函数层面：
     - 加权损失函数，比如对于CELoss，负样本为多样本个数为a，正样本为少样本个数为b，此时正样本的 loss * b/(a + b)，负样本的 loss * a/(a + b)，这样相当于控制模型更加关注少样本对损失函数的影响。通过这种基于类别的加权的方式可以从不同类别的样本数量角度来控制Loss值，从而一定程度上解决了样本不均衡的问题。
   - 其它方法：
     - 选择树类型的ensemble model，会不那么明感对于不平衡问题。
     - 调整阈值修改正负样本比例：通常情况会将大于阈值0.5的样本预测为正样本。这时候我们可以通过调节阈值来调整正负样本的比例，比如设置阈值0.3，大于0.3的样本都判定为正样本，相当于增加了正样本的比例。
     - 利用半监督或者自监督学习。
     - 使用新评价指标，具体参考损失metric部分。

### feature transformer (特征转换)
#### 参考链接：
- [中文数据预处理总结1](https://www.zhihu.com/tardis/zm/art/100442371)
- [中文特征编码总结1](https://zhuanlan.zhihu.com/p/441284257)
- [中文特征编码总结2](https://zhuanlan.zhihu.com/p/67475635)
- [英文总结特征缩放](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)
- [中文总结特征缩放1](https://zhuanlan.zhihu.com/p/95726841)
- [中文总结特征缩放2](https://www.jianshu.com/p/95a8f035c86c)
#### 知识点提炼:
1. 特征编码，有时候对于categorical特征我们的特征分布值比较多或者numerical特征分布很广平，我们需要做一些特征转换方便后续model使用。
   - 分箱特征二元化/多元化，特征二元化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。多元化同理在类别型的属性上选择某几个值归为一个新的属性值。或者在数值型的属性上划分多个区间。这可以让模型只关注少数几个类别而不是有限的多个类别，同时可以对异常值不明感。
   - 独热编码（One-Hot Encoding）独热编码采用N位状态寄存器来对N个可能的取值进行编码，每个状态都由独立的寄存器来表示，并且在任意时刻只有其中一位有效。独热编码的优点：能够处理非数值属性，在一定程度上扩充了特征，编码后的属性是稀疏的，存在大量的零元分量。
   - 对于多属性的类别型特征，我们可以转化成数值型特征，比如采用频率编码，TF-IDF编码，或者embedding，详细参考上面的链接。
   - 对于会连续变动的数值型特征，我们可以转化成比例，这样可以保证无论数值如何改动做预测的时候特征的含义都不会造成影响。
2. 特征缩放，一般基于我们的model选择，我们需要对数值型特征做一些标准化。比如对于基于gradient descend，distance的model或者PCA之类的需要计算cov matrix的model，我们需要标准化。对于树类型的model或者discriminate model（LDA，NB）我们并不需要。另外的原因包括，需要消除样本不同属性具有不同量级时的影响：数量级的差异将导致量级较大的属性占据主导地位，数量级的差异将导致迭代收敛速度减慢，依赖于样本距离的算法对于数据的数量级非常敏感。归一化后求优过程范围变小，寻优过程变得平缓，更容易正确收敛到最优解。
   - 标准化（Standardization）：减去均值除于方差，也就是将数据变为均值为0方差为1的正态分布（注意，这里指的是数据的分布，数值不一定在0-1区间，区分归一化）。标准化在建模过程中，使得各个特征维度无差别对待，消除一些数值尺度差异带来的重要偏见。经过上述处理的数据，能加快模型训练速度，促进收敛；标准化有一个很重要的性质：线性变换不会改变原始数据的数值排序。标准化处理本质上是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是标准化的前提。
   - 归一化（Normalization）：把数据大小限定在特定的范围，比如一般都是 [0,1] 或 [-1,1]，转化成[0,1]区间的同时也叫 Min-Max scaling。归一化主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。其次把有量纲表达式变成无量纲表达式意思是去除单位的意义，便于不同单位或量级的指标能够进行比较和加权。
   - 一般来说优先使用标准化。对于输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。如果数据较为稳定，不存在极端的最大最小值，也可以先用用归一化。如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。
3. 特征编码总结
   - 数值特征
     - 连续数据
       - 直接使用
       - 特征缩放后再使用，包括apply log，ratio之类
     - 离散数据
       - 稀疏，多于20个以上类别，比如ID
         - embedding，一般选择5-10的维度即可
       - 不稀疏，少于20个之内的类别，比如年龄
         - 分桶后进行 one-hot
   - 类别特征
     - 稀疏，多于100个以上类别，比如city/country/language
        - embedding，一般选择5-10的维度即可
     - 不稀疏，少于100个之内的类别
       - 少于10个之内的类别，比如性别
         - 直接 one-hot
       - 多于10个但是少于100个之类的，比如week，device
         - 分桶后进行 one-hot
   - 文本类
     - 句子或者更长的文章
       - 用预训练好的LLM，比如BERT来进行embedding
     - 几个关键词或者词组
       - TF-IDF
       - Word2Vec


## feature selection (特征选择)

### feature reduction (特征降维)
#### 参考链接：
- [中文数据预处理总结1](https://www.zhihu.com/tardis/zm/art/100442371)
- [英文总结维度灾难](https://www.analyticsvidhya.com/blog/2021/04/the-curse-of-dimensionality-in-machine-learning/)
- [中文总结特征降维及提取方法](https://zhuanlan.zhihu.com/p/43225794)
- [中文总结降维及例子](https://zhuanlan.zhihu.com/p/82417864)
#### 知识点提炼:
1. 为什么要降维：
   - 数据维度越高，则需要更大的数据量来进行数据表示，数据量指数级增大。
   - 特征维度越高，学得的模型越复杂，更容易出现过拟合的问题。
   - 高维情况下，容易出现数据样本稀疏问题。
   - 高维空间会给距离计算带来很大困难。
   - 高位空间的距离变的没有意义，对于高纬度的数据直接做基于距离上的模型是毫无意义的。
   - 减少冗余特征或噪声数据。
   - 计算量更小，训练速度更快。
   - 数据存储所占空间（内存）减少。
   - 若将维度降为二维或三维，则方便可视化。
   - 把原本稀疏的全局空间分布边的更加紧凑，利于下游任务拟合model。
2. 常见地降维方法：
   - 线性降维：
     - PCA，无监督学习，PCA着重描述特征，PCA变换矩阵是正交的。具体细节参考PCA章节。
     - LDA，有监督学习，LDA着重抓住其判别特征，LDA一般不是正交的。
   - 非线性降维：
     - T-sne，主要用于可视化了解大概数据分布的整体形状为下游任务准备。具体细节参考T-sne章节。
     - UMAP，同T-sne原理，不过更加适用于提取global信息和可以适用于降到任意维度相比较与T-sne。
     - Autoencoder，采用神经网络的方式，两层编码，两层解码，decode 的最后一层节点个数要跟输入维度一致，原始向量经过编码器和解码器，得到最终输出，将输出与输入比较就可以计算误差（比如取 MSE）。当模型训练完毕，去掉所有 decode 层，就是一个降维的模型了，也就是encode的最后一层是降维结果。

### feature extraction (特征提取)
#### 参考链接：
- [中文总结特征降维及提取方法](https://zhuanlan.zhihu.com/p/43225794)
- [中文特征工程中的特征选择讨论](https://www.zhihu.com/question/28641663)
#### 知识点提炼:
1. 人工选择重要特征，需要比较多的domain knowledge或者之前做实验的experience。
2. 缺失值比率（Missing Value Ratio），如果缺失值较高并且此特征并没有太多作用通过domain knowledge，那么我们可以直接删除此特征，阈值可以设置为10%。
3. 低方差过滤（Low Variance Filter），如果特征的数值基本一致，也就是它的方差非常低，和上一种方法的思路一致，我们通常认为低方差变量携带的信息量也很少，所以可以把它直接删除。实践中，就是先计算所有变量的方差大小，然后删去其中最小的几个。需要注意的一点是：方差与数据范围有相关性，因此在采用该方法前需要对数据做归一化处理。一般是放到同一个range区间内，而不是标准化不然方差将变的都一样。
4. 高相关过滤（High Correlation filter），如果两个变量之间是高度相关的，这意味着它们具有相似的趋势并且可能携带类似的信息。同理，这类变量的存在会降低某些模型的性能（例如线性和逻辑回归模型）。为了解决这个问题，我们可以计算独立数值变量之间的相关性。如果相关系数超过某个阈值，就删除其中一个变量。一般来说如果两个特征相关性大于0.5就要考虑是否是否要删除一个变量。
5. 反向特征消除（Backward Feature Elimination），一般来说我们用线性model来执行这个过程，因为线性model更快，以下是反向特征消除的主要步骤： 
   - 先获取数据集中的全部n个变量，然后用它们训练一个模型。
   - 计算模型的性能。
   - 在删除每个变量（n次）后计算模型的性能，即我们每次都去掉一个变量，用剩余的n-1个变量训练模型。
   - 确定对模型性能影响最小的变量，把它删除。
   - 重复此过程，直到不再能删除任何变量。
6. 前向特征选择（Forward Feature Selection），同反向的原理，只是反过来找到能改善模型性能的最佳特征，而不是删除弱影响特征。不过前向特征选择和反向特征消除耗时较久，计算成本也都很高，所以只适用于输入变量较少的数据集。以下是前向特征选择主要步骤：
   - 选择一个特征，用每个特征训练模型n次，得到n个模型。
   - 选择模型性能最佳的变量作为初始变量。
   - 每次添加一个变量继续训练，重复上一过程，最后保留性能提升最大的变量。
   - 一直添加，一直筛选，直到模型性能不再有明显提高。
7. 嵌入式特征提取，这里主要指靠model进行的特征选择model拟合阶段进行。
   - 基于惩罚项的特征选择法，使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。比如logistics regression中使用L1或者L2或者都用进行特征限制和选择。
   - 树类型的model，都可以自己计算每个feature的importance，具体怎么计算参考树类型的model章节，我们可以根据importance的排序从上到下选择我们想要的特征。


## 面试问题总结：
1. 机器学习中，为何要经常对数据做归一化（Normalization，这里的 Normalization 其实是特征转换里面的 Standardization 的意思）？
   - 归一化后加快了梯度下降求最优解的速度。![如图](/pics/bn_effection.png)
     - 如上图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。
   - 归一化有可能提高精度。
     - 一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如实际情况是值域范围小的特征更重要）。
   - 归一化可以避免过拟合和梯度消失。
     - 归一化常在DL中激活函数前的某一层。因为归一化之后参数的分布范围会被限制住，这样对于有些激活函数（比如：sigmoid）算梯度的时候不会计算到梯度消失（梯度接近0的范围）的区间。
2. 归一化的类型？
   - 线性归一化：
     - 比如最大值最小值缩放，这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。
   - 标准差标准化：
     - 经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
   - 非线性归一化：
     - 经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。
3. 哪些机器学习算法不需要做归一化处理？
   - 本质上来说需要计算梯度和距离的model则需要考虑归一化处理数据。
   - 比如概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
4. 对于树形结构为什么不需要归一化？
   - 数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。归一化并不影响树的分裂也就不影响如何构造树了。
   - 然而对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。
   - 其次就是从统计的角度来看树模型是概率模型，特征变量和预测值之间的条件概率。
5. 连续特征的离散化：在什么情况下将连续的特征离散化之后可以获得更好的效果？
   - 在工业界，很少直接将连续值作为逻辑回归等线性模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，
   - 离散特征的增加和减少都很容易，易于模型的快速迭代；
   - 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
   - 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
   - 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 
   - 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
   - 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以如何划分区间更重要；
   - 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
   - 模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。
   - 参考链接 [link](https://www.zhihu.com/question/31989952)
6. 什么样的模型对缺失值更敏感？
   - 核心思想是对于数据是基于特征采样的统计分布的方法对缺失值有更高的抗性。也就缺失值是随机的(MAR)，或者是完全随机的(MCAR)，表示参数的分布与观测到还是没有观测到数据没有关系。这一类的模型，当有缺失值的影响，并且经过处理以后对结果影响不大。
   - 树模型能将缺失值尽量解释为其它特征，从而对于缺失值的敏感度较低，最适用于应对缺失值。
   - 其它统计分布的方法（如广义加性模型GBDT和KNN）能在尽量保证“缺失值是随机的”的前提下降低缺失值造成的负面影响，机理是预先填充缺失值并在训练中视其有值。
   - 基于数据的值来定位边界的方法（如SVN）对缺失值的抗性不好，因为不恰当的非随机缺失值可能导致模型出现意外。
   - 涉及到距离度量(distance measurement)时，如计算两个点之间的距离，缺失数据就变得比较重要。因为涉及到“距离”这个概念，会影响模型的拟合，那么缺失值处理不当就会导致效果很差，比如KNN，SVM。不过这里的前提是不恰当的非随机缺失值很多或者处理不恰当。
   - 对于KNN虽然是基于距离的度量模型但是比较trick的是，它仍然是一个基于统计的模型，对缺失值仍然是比较robust的。对于样本较为充分的特征，KNN可以接受在训练开始前处理缺失值，并且一般导致意外的损失的概率较低。
   - 神经网络的鲁棒性虽然强，但是不一定可以处理好缺失值，因为边界点上的值的非MCAR缺失会造成梯度判断的方向性错误。
   - 贝叶斯模型对于缺失数据比较稳定，数据量很小的时候首推贝叶斯模型。
   - 总结，对于有缺失值，并且经过处理以后的数据：
     - 数据量很小，用朴素贝叶斯。
     - 数据量适中或者较大，用树模型，优先 xgboost。数据量小不用树模型的另一个主要原因是XGB，LGB模型很容易过拟合。
     - 数据量较大，也可以用神经网络，前提是是缺失特征有足够的样本并且在训练开始前处理缺失值。
     - 避免使用数据的值来定位边界的方法相关的模型，如SVM。
     - 参考[链接](https://www.zhihu.com/question/58230411)
7. 如何处理特征之间的共线性？
   - 本质上是特征选择的问题，再具体一定来说，实际上是你的数据本身包含得真正有用的特征信息少于数据集里面的特征信息数量。参考上面写的特征降维和特征提取的过程。
   - 降维：
     - 如果使用PCA降维的话，先说结论，结论是可以起到效果，不过极端例子是所有特征都是线性相关，则只有一个特征有用，并且此特征的component有99%的variance explained。此时降维只能告诉你你的数据信息其实很少，或者把所有的特征整合到一个特征里，达到数据的transform，并不能真正去除共线性的问题，如果移除共线性得到更多的特征信息是一开始的goal的话。
   - 数据角度：
     - 计算出特征之间的相关性之后，如果发现有的特征之间相关性太高了比如大于0.7，依赖domain knowledge或者经验，直接drop掉。
     - 增加样本容量，可以帮助更准确的计算相关性并且降低特征的variance。
   - 模型角度：
     - 利用L1回归，限制参数的大小，来达到特征选择，移除共线性的特征权重。
     - 利用反向或者前向特征选择步骤来达到特征选择。或者树模型的特征重要性来选择特征。
8. 添加数据和添加特征对variance和bias的影响分别是什么？
   - 添加数据，也就是增加data size，可以使训练数据变的含有更多的信息并且抗噪，所以会降低variance。
   - 添加特征，也就是增加更多描述当前数据集的信息，所以可以降低bias但是容易过拟合增加variance。
