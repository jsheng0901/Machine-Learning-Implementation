## 全连接层 - Fully connected dense layers
### 基本原理
参考链接：
- [中文原理分析和计算图推导](https://ver217.github.io/2018/07/06/fc/#more)
- [中文原理详解1](https://www.cnblogs.com/pinard/p/6418668.html)
- [中文原理解释2](https://zhuanlan.zhihu.com/p/518638598)
- [斯坦福DL课件讲解](https://stanford.edu/~shervine/teaching/cs-230/)
- [pytorch中相关应用文档](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)
### 优点
- 克服了人工设计特征费时、费力的缺点。
- 通过逐层数据预训练得到每层的初级特征。
- 分布式数据学习更加有效（指数级）。
- 相比浅层建模方式，深层建模能更细致高效的表示实际的复杂非线性问题。
### 缺点
- 参数可能很多很大，容易过拟合。
### 知识点提炼
- 参数解释
  - init，初始化参数的方式，这里初始化方式会影响后续的参数倒数，可能造成梯度爆炸或消失，对于不同激活函数初始化方式应该不一样，因为不同的激活函数梯度函数不一样。
  - n_out，输出结果维度，也就是此层神经元的维度。
  - act_fn，激活函数的方式，一般全连接层采用relu作为激活函数，具体情况具体分析，具体参考激活函数文档。
  - optimizer，优化器的方式，每次反向传播后要更新此层的参数，不同的优化器采用不同的方式更新参数，具体参考优化器文档。
- 其它要点 
  - 全连接的核心操作是矩阵乘法，本质上是把一个特征空间线性变换到另一个特征空间。实践中通常是把特征工程（或神经网络）提取到的特征空间映射到样本标记空间，参数w相当于做了特征加权。
  - 不同的module采用的FC层会起到不同的效果，比如在分类任务中FC输出层主要是线性变化到输出维度空间，再比如在transfer learning中FC主要可充当”防火墙”，用来过滤掉目标和源域差别太大的参数，也可以理解为参数提取。具体如何构建需要参考不同的module。
  - FC层可以构建最简单的DNN及深度神经网络，不过越深越容易过拟合，每一层的泛化能力会越来越弱，一般不推荐使用，对于简单的数据我们可以考虑传统ML，对于复杂的数据，我们更多的参考具体task的model，FC层只起到中间一部分的作用。
  - 每FC层 dx, dw, db 只和同一层的线性输出 z, 此层的输入 x, 此层的参数 w 和 此层的损失函数对于输出结果的梯度 dL/dy 相关。
### Engineer Work
- 实际工作中一般作为某个module的某一层，直接调用Pytorch里面的linear layer，或者TensorFlow里面的dense layer。

## RNN
### 基本原理
参考链接：
- [中文简介及RNN变体介绍](https://zhuanlan.zhihu.com/p/28054589)
- [中文原理详解](https://www.cnblogs.com/pinard/p/6509630.html)
- [中文RNN系列讲解](https://yzhihao.github.io/machine%20learning/2017/03/12/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html)
- [英文RNN系列讲解包含反向传播推导和LSTM/GRU实现](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/)
- [RNN反向传播公式推导](https://jramapuram.github.io/ramblings/rnn-backrpop/)
- [RNN反向传播推导及代码实现](https://songhuiming.github.io/pages/2017/08/20/build-recurrent-neural-network-from-scratch/)
- [Pytorch官方文档及讲解](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)
### 优点
- 对比传统的神经网络将每个观察视为独立的，RNN通过包含数据点之间的依赖关系将记忆的概念引入神经网络。
- 由于其短期记忆，RNN可以处理顺序数据并识别历史数据中的模式。
- 此外，RNN能够处理不同长度的输入。
- 对比传统的神经网络，在RNNs中，每输入一步，每一层各自都共享参数U,V,W。其反映着RNNs中的每一步都在做相同的事，只是输入不同，因此大大地降低了网络中需要学习的参数。
- 同时共享参数达到了传递记忆的功能。参数的隐藏层能够捕捉序列的信息。
### 缺点
- RNN存在梯度下降消失的问题。在这种情况下，用于在反向传播期间更新权重的梯度变得非常小。将权重与接近于零的梯度相乘会阻止网络学习新的权重。停止学习会导致RNN忘记在较长序列中看到的内容。梯度下降消失的问题随着网络层数的增加而增加。
- RNN只有短期记忆而没有长期记忆。因为RNN仅保留最近的信息，比较远的信息比如对首个输入的记忆会因为梯度消失的问题导致无法传递到后面。
- 由于RNN使用反向传播及时更新权重，如果使用Sigmoid或者Tanh激活函数初始化参数的时候可能会导致网络遭受梯度爆炸的影响，如果使用ReLu激活函数，则会受到死亡ReLu单元的影响。前者可能会导致收敛问题，而后者会导致停止学习。
### 知识点提炼
- 参数解释
  - init，初始化参数的方式，这里初始化方式会影响后续的参数倒数，可能造成梯度爆炸或消失，对于不同激活函数初始化方式应该不一样，因为不同的激活函数梯度函数不一样。
  - n_out，输出结果维度，也就是此层RNN输入参数和隐藏层参数神经元的维度，RNN里面两个参数的输出维度都一样。
  - act_fn，激活函数的方式，一般RNN采用 Tanh 作为default激活函数，但是RNN很容易遇到梯度爆炸和消失，所以其实使用 Relu 作为激活函数更容易避免梯度爆炸和消失，不过具体情况参考激活函数文档。
  - optimizer，优化器的方式，每次反向传播后要更新此层的参数，不同的优化器采用不同的方式更新参数，具体参考优化器文档。
- 其它要点 
  - RNN通过神经网络中的反馈回路实现记忆，这其实是RNN与传统神经网络的主要区别。反馈回路允许信息在层内传递，而前馈神经网络的信息仅在层之间传递。
  - RNN的反向传播求导，是要往后对每一个timestep里面的参数都要求导，所以简化来说梯度会等于act'^n * w ^ n，而此时如果w很大，比如W初始化为大于1的数，则很容易梯度爆炸，相反如果w很小，小于1的数，则很容易梯度消失。特别当激活函数是Sigmoid或者Tanh这一类两端导数接近0的函数。
  - RNN解决了DNN的两个问题：第一，无法对时间序列数据进行建模处理；第二，全连接的操作导致参数膨胀。RNN里面每输入一步，每一层各自都共享参数U,V,W。其反映着RNNs中的每一步都在做相同的事，只是输入不同，所以可以起到记忆作用，同时因此大大地降低了网络中需要学习的参数。
  - 对于梯度爆炸，我们可以采用梯度剪切或者L1/L2正则化。
  - 对于梯度消失两个维度来改变： 
    - 从网络结构上来改动，可以采用残差网络连接，或者选择不同的layer设计，用LSTM/GRU的门设计来控制，具体参考LSTM文档。
    - 从数据分布上来改动，Batch-normal控制反向传播中参数放大缩小的影响，调整激活函数为Relu，或者初始化参数范围根据激活函数选择。
### Engineer Work
- 实际工作中一般作为某个RNN网络中的某个timestep的cell。直接调用Pytorch里面的nn.RNN。

## LSTM
### 基本原理
参考链接：
- [中文原理解析及例子讲解](https://zhuanlan.zhihu.com/p/141853644)
- [中文原理及参数计算介绍](https://zhuanlan.zhihu.com/p/388531465)
- [中文LSTM理解及反向传播公式推导](https://zhuanlan.zhihu.com/p/377727084)
- [中文LSTM介绍及反向传播公式推导](https://www.cnblogs.com/pinard/p/6519110.html)
- [中文深入浅出LSTM](https://www.zhihu.com/tardis/zm/art/104475016)
- [中文LSTM各方面总结](https://zhuanlan.zhihu.com/p/406408470)
- [英文RNN/LSTM/GRU系列讲解带动画](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
- [英文LSTM可视化介绍及具体应用结合](http://blog.echen.me/2017/05/30/exploring-lstms/)
- [英文LSTM图文介绍](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Pytorch官方文档及讲解](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)
- [英文解释为什么LSTM可以缓解梯度消失1](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)
- [英文解释为什么LSTM可以缓解梯度消失2](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)
### 优点
- RNNs可以视为一个所有层共享同样权值的深度前馈神经网络。它很难学习并长期保存信息。于是增大网络存储的想法随之产生。采用了特殊隐式单元的LSTM便是为了长期地保存输入。一种称作记忆细胞的特殊单元类似累加器和门控神经元：它在下一个时间步长将拥有一个权值并联接到自身，拷贝自身状态的真实值和累积的外部信号。
- LSTM是RNN的一个变种模型，继承了大部分RNN模型的特性和优点，同时解决了梯度反传过程由于逐步缩减而产生的梯度消失问题。
### 缺点
- RNN的梯度问题在LSTM及其变种里面得到了一定程度的解决，但还是不够。它可以处理100个量级的序列，而对于1000个量级，或者更长的序列则依然会显得很棘手。
- LSTM计算费时。每一个LSTM的cell里面都意味着有4个全连接层(MLP),如果LSTM的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。
- LSTM在并行处理上存在劣势。其内部四个门直接存在一定的关联性，无法并行计算处理。应该说RNN网络的特性决定了它们不能很好地并行化处理数据。
### 知识点提炼
- 参数解释
  - init，初始化参数的方式，这里初始化方式会影响后续的参数倒数，可能造成梯度爆炸或消失，对于不同激活函数初始化方式应该不一样，因为不同的激活函数梯度函数不一样。
  - n_out，输出结果维度，也就是此层LSTM输入参数和隐藏层参数神经元的维度，LSTM里面四个门对应的两个参数的输出维度都一样。
  - optimizer，优化器的方式，每次反向传播后要更新此层的参数，不同的优化器采用不同的方式更新参数，具体参考优化器文档。
  - act_fn，需要注意的是LSTM的内部激活函数是固定的，分别是Sigmoid和Tanh，也就是说并没有act_fn这个参数可以设置。
- 其它要点 
  - LSTM的门都是Sigmoid函数作为激活函数，只有细胞层的内置C~由Tanh函数配合控制有哪些新信息被加入。使用Sigmoid的主要原因是，可以缩放到0-1的空间，接近0的会被遗忘，接近1的会被记住并传递下去。
  - LSTM由遗忘门，记忆门，输出门来控制。遗忘门控制之前隐藏层和当前输入中需要被遗忘的有哪些，同时影响当前的细胞层。记忆门的作用与遗忘门相反，它将决定新输入的信息中哪些信息将被保留。记忆门内部由输入门和当前细胞层控制当前需要被记住的信息。细胞层由遗忘门和记忆门控制，是LSTM的核心，它控制的长期记忆和当前输入信息哪一些需要被留下来传递给下层，输出门控制被遗忘和记忆过的隐藏层信息并用于传递给下一层。
  - LSTM总共参数个数由四个包含MLP的参数控制，参数数量 = 4 * ((词向量的维度 + lstm神经元个数） * lstm神经元个数 + lstm神经元个数)。词向量的维度 * lstm神经元个数 -> x_t * w_x，lstm神经元个数 * lstm神经元个数 -> h_t-1 * w_h，lstm神经元个数 -> b。
  - LSTM有很多变体，及可以双向，可以多层，可以seq_2_seq等，具体参考上面的参考链接。
### Engineer Work
- 实际工作中一般作为某个RNN网络中的某个timestep的cell。直接调用Pytorch里面的nn.LSTM，需要注意的是这里可以直接使用双向的LSTM。


## 面试问题总结
1. 简单介绍一下RNN、LSTM、GRU？他们的区别和联系是什么？
    - RNN即Recurrent Neural Networks、循环神经网络，本质是一个全连接网络，但是因为当前时刻受历史时刻的影响。RNN出现梯度消失和梯度爆炸主要体现在长句子中，因为在后向传播BP求导时，当前t时刻隐层输出的梯度包含了所有后续时刻激活函数导数的乘积，所以如果t越小、句子越长，就会出现问题。如果激活函数的导数特别小，累乘就会更小，则会出现梯度消失问题；反之，则是梯度爆炸问题。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。
    - LSTM即Long Short Term Memory、长短时记忆模块，是对RNN存在的梯度消失、梯度爆炸问题的一种优化模型。
    - GRU即Gated Recurrent Unit、门控循环单元，相当于是LSTM的一种变种，将三个门变成了两个门，本质区别不是很大，而且哪个更好用大概率是看实验结果。GRU更快参数更少因此更容易收敛。但是数据集很大并且长距离句子多的情况下LSTM门的信息控制比GRU更好，同时表达性能更好。
    - 联系：三个都是NLP里面常见地处理t时刻单词的layer，都是处理时间序列关系的数据，都不能并行运算，当前输入依赖前一个输出。
    - 区别：主要是内部的门不一样，对之前信息通过此layer后的控制不一样。
2. LSTM是怎么解决梯度消失的问题的？
   - 核心思想是BPTT的方向传播中LSTM可以对累计的激活函数导数乘积可控，通过遗忘门控制梯度的范围，然而RNN是不可控的只能一直累计，所以会有可能梯度落到激活函数导数的平滑位置导致梯度消失。
   - 具体数学推导参考 [link1](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)和[link2](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)。
3. 有哪些其它的解决梯度消失或梯度爆炸的方法？
   - 梯度爆炸，核心思想都是控制梯度的范围：
     - 梯度裁剪gradient clipping，当BP时的梯度小于某个阈值或大于某个阈值时 ，直接裁剪，防止太小的梯度累乘带来的梯度消失或太大的梯度累乘带来的梯度爆炸。
     - L1/L2正则化控制梯度范围。
   - 梯度消失，核心思想都是维护之前的梯度：
     - 从神经网络结构上改动：
       - 采用残差网络连接，跨层的连接结构能让梯度无损的进行后向传播。
       - 选择不同的layer设计，用LSTM/GRU的门设计来控制。
     - 从控制数据分布上来改动：
       - Batch-normal控制反向传播中参数放大缩小的影响。相当于对每一层的输入做了一个规范化，强行把这个输入拉回标准正态分布N~(0,1)。这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数的大变化，进而梯度变大，避免产生梯度消失问题。而且梯度变化大 意味着学习收敛速度快，加快模型的训练速度。
       - 改变激活函数，例如减少使用sigmoid，tanh这类存在平滑导数区间的激活函数，改成使用Relu、LeakRelu等存在constant导数的激活函数。
       - 初始化参数范围根据激活函数选择，从而达到控制参数的分布范围。
4. Residual connections的原理是什么？
   - 为什么需要：深度网络存在退化问题，例如56层的网络反而没有20层的网络效果好，假设后36层仅是恒等映射也至少能达到20层的效果，说明了正是因为这种退化问题导致，深度网络的训练是很难的。 
   - 怎么做：本质是跨层的网络连接结构，例如基础的网络结构是x-->f(x)-->loss，而梯度的传播是逆向的，如果梯度特别小，随着网络的后向传播就会出现梯度消失的问题。但现在的残差网络的结构是x-->f(x)+x-->loss，就算梯度后向传播到f(x)再到x时特别小也不用担心，因为还有一条路径是梯度直无损的传到x。
   - <mark>TODO: 需要数学推导一遍残差网络的反向传播。</mark>
5. Batch Normalization和Layer Normalization的区别？
   - BN: 对每个样本的同一个维度做归一化。例如2D -> [N, F]，BN的mean和var是 -> [1, F]，例如3D -> [N, L, F ]，BN的mean和var是 -> [1, L, F]。
   - LN：对单一样本的的所有特征维度的归一化。例如2D -> [N, F]，LN的mean和var是 -> [N, 1]，例如3D -> [N, L, F ]，LN的mean和var是 -> [N, L, 1]。
   - LN比BN更适用于时间序列的input，因为LN是对每个样本所有维度做标准化，而BN是对所有mini-batch里面的样本的同一个维度做标准化，时间序列的input长度可能都不一样，对不同样本做的话，如果样本长度变化比较大的时候，每次计算小批量的均值和方差，均值和方差的抖动大，并且有时候可能因为句子长度不一样，同一个mini-batch里面有的是单词有的对应的是padding，这样的归一化没有意义。全局的均值和方差：测试时遇到一个特别长的全新样本，训练时未见过，训练时计算的均值和方差可能不好用。LN每个样本自己算均值和方差，不需要存全局的均值和方差。更稳定，不管样本长还是短，均值和方差是在每个样本内计算。还有一个原因是LN是对同一个样本做，这样并没有破坏一个句子的内部语义联系，然后BN是跨样本做，没有任何意义对不同的单词同一个维度做标准化，反而会破坏句子的内部联系。
   - 图片展示LN和BN的区别。![BN_LN](/pics/bn_vs_ln.png)