### 基本原理
参考链接：
- [中文原理讲解及详细解释LGB](https://zhuanlan.zhihu.com/p/99069186)
- [中文讲解源论文深度解析LGB](https://blog.csdn.net/anshuai_aw1/article/details/83048709)
- [中文详细讲解LGB与XGB的区别及例子解释](https://zhuanlan.zhihu.com/p/366952043)
- [中文对比解释XGB与LGB的区别和相同点包含原理公式推导](https://zhuanlan.zhihu.com/p/87885678)
- [LGB官方文档以及各方面详细应用解释](https://lightgbm.readthedocs.io/en/stable/index.html)
### 优点
- 对比XGB速度方面：
  - 采用了直方图算法将遍历样本转变为遍历直方图，极大地降低了时间复杂度。
  - 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算。
  - 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要地计算量及不必要的节点分裂。
  - 寻找类别特征最优分裂点采用 many-vs-many 的模式分裂节点，因为one-vs-rest 模式，每次只能根据一个类别做分类，这种模式效率比较低，而且不利于决策树学习。
  - 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略。
  - 对缓存也进行了优化，增加了缓存命中率。
- 对比XGB内存方面：
  - XGB使用预排序后需要记录特征值及其对应样本的统计值的索引，而LGB使用了直方图算法将特征值转变为bin值，且不需要记录特征到样本的索引，将空间复杂度从O(2 * #data)降低为O(#bin)，极大地减少了内存消耗。
  - 采用了直方图算法将存储特征值转变为存储bin值，降低了内存消耗。
  - 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。
- 其它优点继承GBD和XGB所有优点。LGB更像是工程上多样本多特征下XGB的速度更快、内存占用更低版本。
### 缺点
- 采用直方图方式找最佳分裂点时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去。可能会造成精度上的损失，不过这里可以配合增加拟合次数来达到同样的精度，并且距作者说一定范围的损失精度，反而起到降低复杂度，提高对梯度大的数据的后续拟合能力，从而达到防止过拟合的效果。
- Leaf-wise 算法的增长策略构建树，可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。同时每一轮构建一个更简单的树有一定地达到防止过拟合的效果。
- 同样继承了Boosting的统一缺点，迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LGB是基于偏差的算法，所以会对噪点相对较为敏感。
### 知识点提炼
- 参数解释
  - num_iterations，构建多少个并行的DT，一般来说LGB每棵树都很简单，所以总共多少棵树一般选择比较大的值比如200，然后配合early stopping防止过拟合。
  - data_sample_strategy，每次构建树的时候样本的采样方式，有 bagging 和 goss 两种方式。Bagging 对应随机有放回的抽样，和RF一样。GOSS 则是LGB独有的基于梯度的单边梯度算法用来过滤掉梯度小的样本。
  - bagging_fraction，当使用bagging来采样的时候，设置的比例。
  - feature_fraction，同理对特征进行随机采样的比例在每次构建树之前。
  - num_leaves，每棵树的最多叶子个数。因为LGB采用了基于 Leaf-wise 算法的增长策略构建树，所以不仅仅可以用层数来限制树还可以用叶子个数。
  - tree_learner，对应LGB独有的三种worker并行的方式加不并行单一worker，分别是feature，data，voting和serial。
  - max_depth，单个决策树的最大深度，和减枝控制过拟合有关，如果不设置，则一直build树到底。这个参数同样很重要对于LGB，特别是小样本的数据，因为我们会构建很多树，所以单个树越简单越好来避免过拟合，一般设置4左右。不过无论如何都是走 Leaf-wise 策略构建树。
  - min_data_in_leaf，单个决策树节点构建叶子结点的时候，叶子结点最少需要多少个样本来构建。如果分裂后低于这个参数则意味着不再分裂此节点。此参数可能会影响回归任务的平滑效果。
  - min_gain_to_split，单个决策树节点最小需要得到多少gain才会继续分裂，如果分裂后低于此参数则意味着不再分裂此节点。此参数可以达到early stopping的效果，防止过拟合太多的单一决策树。
  - min_data_in_bin，最小有多少个数据在一个直方图的bin里面，LGB独有的基于直方图的方法构建特征样本参数，可以有效控制一个bin一个样本从而达到控制过拟合。
  - enable_bundle，是否使用互斥特征捆绑及EFB算法，默认是true，可以有效加速训练对应稀疏数据。
  - learning_rate，每个单一决策树对于最终结果的贡献权重，缩小单一树对于最终结果的共享。和n_estimators参数是一组相互牵制的参数，相互之间有trade-off。
  - 其它常见参数参考XGB。
- 损失函数解释
  - 损失函数可以是自定义的，只要满足符合一阶导数和二阶导数即可。LGB在梯度方面和XGB一样。
  - 回归问题：
    1. 均方差损失：(y-f(x))^2，适用于大部分数据分布均匀的情况。
    2. Huber损失：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。
    3. 分位数损失：对训练集进行分段预测的时候，则采用quantile。
  - 分类问题：
    1. 指数损失：exp(-yF(x))，类似Adaboost。
    2. 对数损失：交叉嫡损失，主要用于二分和多分类问题。
- 正则化解释
  - Shrinkage，对应参数里面的learning_rate，及我们每次对每棵树的拟合进行缩放。实践中一般把此参数设置很小，比如0.01之类的，然后配合early stopping来使用。
  - Subsample，子采样，取值为 (0,1]。注意这里的子采样和XGB不一样，这里可以选择两种方式，一种和XGB一样基于bagging的采样，另一种是GOSS，需要设置 top_rate 和 other_rate，default情况下top = 0.2 other = 0.1，分别对应公式里面的 a * 100%, b * (1 - a) * 100%。
  - Early Stopping，对应参数里面的tol，机器学习迭代式训练模型中很常见地防止过拟合技巧，具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。
  - 与GBDT最大的不同是LGB也和XGB一样在目标函数里面就加入了正则化，正则项里包含了树的叶子节点个数、叶子节点权重的L2或L1范式。这样每次增加分裂都能有一定的penalty，有效地控制了树的复杂度。
  - 列抽样也能有效的降低每次树拟合的复杂度，达到降低variance的效果。
  - 直方图方式构建特征，虽然是依据最优切分变量，没有考虑全局最优解，但是这种误差配合 num_iterations 和其它控制树的复杂度的参数反而起到一定的正则化的作用。
  - 其它正则化的方法就是对每次拟合的单一CART树进行限制，此处参考decision tree里面的note详细介绍了单一树的正则化。
- 其它要点 
  - 大部分单一树的要点可以参考decision tree里面的note。
  - LGB使用了基于直方图的决策树算法，这一点不同于XGB中的贪心算法和近似算法，直方图算法在内存和计算代价上都有不小优势。
    - 1）内存上优势：很明显，直方图算法的内存消耗为 O(#data * #features * 1Bytes) 因为对特征分桶后只需保存特征离散化之后的值，而XGB的贪心算法内存消耗为：O(2 * #data * #features * 4Bytes)，因为XGB既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。
    - 2）计算上的优势：预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为 O(#data * #feature)，而直方图算法只需要遍历桶就行了，时间为 O(#bin * #feature)。
  - XGB采用的是level-wise的分裂策略，而LGB采用了leaf-wise的策略，区别是XGB对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGB也进行了分裂，带来了不必要的开销。Leaf-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。
  - LGB使用直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。
  - LGB支持类别特征，不需要进行独热编码处理。并且对于类别特征支持多对多的分裂，而不是一对多的分裂方式，这里很重要，因为大部分高纬度的大数据是很稀疏的，然而one-hot之后对于特征很多的类别特征，构建树的时候很容易造成样本分布不平衡地树，此时的gain得到的意义很小对于都是0而少量的1的情况。同时也会造成模型精度的影响。
  - LGB优化了特征并行和数据并行算法，除此之外还添加了投票并行方案。
  - LGB采用基于梯度的单边采样来减少训练样本并保持数据分布不变，减少模型因数据分布发生变化而造成的模型精度下降。
  - LGB采用特征捆绑，转化为图着色问题，减少特征数量。
  - 总的来说，LGB是工程上多样本多特征下XGB的速度更快、内存占用更低版本。
### Engineer Work
- 一般来说直接调用sklearn中的LGB接口来实现。实际生产中普通版本目前来说在(440, 10)的数据集上每棵树拟合需要6s左右在8cores/16Memory上。
- 如需要再1M以上的数据跑production，目前本人测试过的情况是在GCP上1M的数据还是很容易实现进pipeline的。
- 如果需要跑更大的数据，推荐参考官方文档里面的分布式训练方法，含解释和第三方spark带GPU的guide。[参考link](https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html)