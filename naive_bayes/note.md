
### 基本原理
参考链接：
- [中文原理讲解及例子解释](https://zhuanlan.zhihu.com/p/37575364)
- [中文原理详细解释](https://blog.csdn.net/qq_32742009/article/details/82017344)
- [中文解释及其例子](https://zhuanlan.zhihu.com/p/624002501?utm_id=0)
- [中文拉普拉斯平滑解释](https://www.python100.com/html/94252.html)
- [英文原理理解](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)
- [sklearn中相关应用文档](https://scikit-learn.org/stable/modules/naive_bayes.html)
### 优点
- 非常容易理解和解释结果和算法过程。
- 朴素贝叶斯算法简单容易实现，对异常值、缺失值不敏感。
- 当数据分布接近相互独立时，分类准确率高，可实现多分类。
- 运行速度很快，对于大规模数据。
- 对于生成式模型，模型可通过增量学习得到。
- 对于生成式模型，模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。
- 当feature是categorical时候一般比numerical情况下performance更好，因为numerical下有强假设normal distribution。
### 缺点
- 对数据的依赖性高，如果训练集误差大，最终效果就不好。
- 需要知道先验概率，收到先验概率准确性的影响。
- 如果各个特征之间依赖性较高，会降低分类效果。
- 对于categorical feature，没有在train过程中看见的feature会assign概率为0，此时会出现zero frequency的情况，需要平滑处理，例如Laplace estimation。
### 知识点提炼
- 参数解释
  - priors，每个class的先验概率。可以不设置，default按照class的频率来assign。
  - type，此参数并不是常规参数，主要是表示选择那种likelihood分布，比如连续数据选择高斯，离散数据选择Multinomial或者Bernoulli。
- 损失函数解释
  - 生成式模型，没有损失函数，此处贴出贝叶斯核心公式。![bayes rule](/pics/bayes_rule.png)
- 正则化解释
  - 无
- 其它要点 
  - 朴素贝叶斯的基本假设是条件独立性，即在类确定时，假设其各个特征相互独立。
  - 如果输入feature不符合高斯，则可以适当的应用数据转化，变成normal distribution。
  - 遇到条件概率相乘为0的时候，使用拉普拉斯平滑，拉普拉斯平滑的参数alpha值选择一般为1，alpha * k代表特征取值的个数，这个值越大，相应的拉普拉斯平滑所增加的概率值也就越小。
  - 朴素贝叶斯的模型比较简单，因为它的假设是强假设并且是很难真正完全实现的情况，所以此假设简化了模型，对异常值和缺失值有较高的容错，属于低方差模型。
  - 对于异常值，由于朴素贝叶斯判别公式中，采用的是连乘的方式，因此个别的异常值，对于整体的结果影响不大。对于类别特征，因为计算P(xi|y)是通过出现频率来计算，此时并不会出现概率偏倚，所以异常值反而可以提高模型的泛化能力。对于连续数值属性的特征，异常值可能会影响特征的概率分布，导致概率分布偏倚，那么就有可能会影响P(xi|y)的结果，不过整体影响可能不大。此外，最终的分类结果，是根据各个类别概率的排序来判别的结果，因此，个别异常值影响也不大。
  - 对于缺失值，在训练的时候，如果某个样本的特征值缺失了，那么计算这个特征在这个类型下的 Likelihood P(xi|y) 的时候此缺失值直接忽略，比如数值类型，计算mean，variance的时候直接忽略此缺失值，类别特征计算出现频率也直接忽略。在测试阶段，如果样本的特征有缺失值，那么此特征对应的 Likelihood P(xi|y)，也将被忽略计算 Likelihood 连成的时候。所以少量的缺失值对模型影响并不大，所以说朴素贝叶斯是缺失值和异常值都不太敏感的模型。
  - 可以进行增量学习，因为训练过程中其实只需要计算出各个特征对应的先检概率和条件概率，这些概率值可以快速的根据增量数据进行更新，并不需要重新全量训练，此特性非常适合于超出内存的大量数据计算和随时间等（流数据）获取数据的计算。
  - 和LR对比，LR更适合大数据，NB更适合小数据。LR可以处理co-linearity用正则化，但NB不行。
### Engineer Work
- 一般来说直接调用sklearn中的NB包来实现。实际生产中普通版本目前来说在(150, 4)的数据集上需要2ms左右在8cores/16Memory上。
- 如需要再1M以上的数据跑production，推荐用spark的版本来实现。[参考例子](https://spark.apache.org/docs/latest/mllib-naive-bayes.html)

### 面试问题总结
1. 为什么朴素贝叶斯如此“朴素”？
   - 因为它假定所有的特征在数据集中的作用是同样重要和独立的。这个假设现实中基本上不存在，但特征相关性很小的实际情况还是很多的，所以这个模型仍然能够工作得很好。
2. 高度相关的特征对朴素贝叶斯有什么影响？
   - 假设有两个高度相关的特征，那么此特征在计算条件概率的时候计算了两次相当于，那么最终结果会向此特征的方向进行偏倚，最终影响计算结果。所以如果应用朴素贝叶斯，我们最好把相关性特征先去除掉。