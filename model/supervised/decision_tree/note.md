
### 基本原理
参考链接：
- [中文原理讲解及例子解释分类决策树](https://blog.csdn.net/aizenggege/article/details/82928367)
- [中文原理讲解分类决策树](https://zhuanlan.zhihu.com/p/537933555)
- [中文解释分类决策树及相关数学推导](https://zhuanlan.zhihu.com/p/59484953?utm_id=0)
- [中文解释回归树](https://zhuanlan.zhihu.com/p/82054400)
- [英文原理理解](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)
- [sklearn中相关应用文档以及详细解释](https://scikit-learn.org/stable/modules/tree.html#decision-trees)
### 优点
- 容易理解和解释结果和算法过程。可以可视化结果解释。
- CART，C4.5可以处理数值和类别的特征。并且可以处理缺失值。
- 对于树类型的模型，不需要对数据做预处理，不需要归一化，不需要标准化数据，也不需要满足什么数据前提假设。
- 预测数据时间是log(n)(n是训练树的时候样本个数)级别，因为我们构建的是类似BST。
- 可以处理多维度输出的分类问题，及多分类问题。
- 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
- 对于异常点的容错能力好，模型更robust。
### 缺点
- 决策树算法非常容易过拟合，导致泛化能力不强。可以通过调参设置节点最少样本数量和限制决策树深度等参数来改进。
- 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
- 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法每次sample样本和特征来改善。
- 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重或者平衡样本类别分类来改善。
- 每次划分只用到单一特征，有时候单一特征并不能很好表现出数据分类。这个可以通过集成学习随机森林来改善。
- 缺乏平滑性（回归预测时输出值只能输出有限的若干种数值）。
- 不适合处理高维稀疏数据。很容易过拟合。因为稀疏数据很容易划分得到很好的纯度，并且不需要很深地树，这样就很容易导致模型过拟合。
- 工程上很难进行并行计算，需要loop完所有特征来找到最佳分裂点。
### 知识点提炼
- 参数解释
  - max_depth，决策树的最大深度，和减枝有关，如果不设置，则一直build树到底。
  - min_samples_split，分裂后子树最小需要多少个样本才可以去分裂一个树节点。
  - min_samples_leaf，分类树节点构建叶子结点的时候，叶子结点最少需要多少个样本来构建。如果分裂后低于这个参数则意味着不再分裂此节点。
  - min_impurity_decrease，当分类树节点最小需要降低多少impurity，如果分裂后低于此参数则意味着不再分裂此节点。
- 损失函数解释
  - ID3，信息增益，当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较偏向取值较多的特征。
  - C4.5，当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。
  - CART分类树，基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。对比上面两个，计算速度更快，同时效果几乎一样。因为我们用除法取代了对数计算，大大降低了计算时间。
  - CART回归树，平方误差最小化。及最小化分裂后区域target值的variance。
- 正则化解释
  - 预减枝，完全正确分类训练集之前，较早地停止树的生长。
    1. 最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长
    2. 到达此结点的实例具有相同的特征向量，而不必一定属于同一类，也可停止生长。
    3. 到达此结点的实例个数小于某一个阈值也可停止树的生长。
    4. 还计算每次分裂对性能的增益，如果这个增益值小于某个阈值则不进行扩展，比如impurity的降低。
  - 后减枝，极小化决策树整体的损失函数。后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。
    1. REP-错误率降低减枝，顾名思义如果减掉某个子树整体错误率并没有变化，则说明可以减掉此子树，注意此处要用测试数据减枝而不能用训练数据。因为训练数据是不可能出现分裂后的错误率比分裂前高的情况。
- 其它要点 
  - 对比预剪枝和后剪枝，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛化性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。
  - CART决策树是二分类树，然而其它两个是多叉树。CART更快更高效。
  - C4.5对于连续特征的处理方式会先把连续数据group起来，分成几个bucket，然后再当离散数据处理划分数据集。
  - 对于缺失值的处理，在分裂的时候把所有缺失值归到左子树或者右子树然后对比哪一种情况下，impurity降低最多，则划分到那个分支。
  - 如果输入训练数据是很sparse的，可以用sparse csc_matrix 在训练前。
  - 总的来说，对于决策树build的时间复杂度一般是O(m * n * log(n) * tree_depth)，对每次split，我们需要遍历所有feature，然后对整个dataset基于特征排序 -> O(n * log(n))，再乘以总共要生成树的深度，predict的时间复杂度是O(log(n))，因为训练完后已经是一个BST，m是feature个数，n是样本个数。
  - 不同类型的决策树对比。![decision_tree](/pics/decision_tree_compare.png)
### Engineer Work
- 一般来说直接调用sklearn中的DT包来实现。实际生产中普通版本目前来说在(150, 4)的数据集上需要0.05s左右在8cores/16Memory上。
- 如需要再1M以上的数据跑production，一般不推荐此算法，model会变的很容易让树build的很大很复杂，非常容易导致过拟合。推荐集成学习比如XGB/LGB。