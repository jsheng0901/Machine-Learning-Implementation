
### 基本原理
参考链接：
- [中文原理讲解及例子解释回归GBDT](https://zhuanlan.zhihu.com/p/81016622)
- [中文原理讲解及例子解释二分类GBDT](https://zhuanlan.zhihu.com/p/89549390)
- [中文原理讲解及例子解释多分类GBDT](https://zhuanlan.zhihu.com/p/91652813)
- [中文解释和基础理解梳理总结GBT](https://zhuanlan.zhihu.com/p/361036526)
- [中文解释集成学习系列总结](https://www.cnblogs.com/massquantity/p/9174746.html)
- [sklearn中相关应用文档以及详细解释](https://scikit-learn.org/stable/modules/ensemble.html)
### 优点
- 每一个树都是CART树，容易理解和解释结果和算法过程。可以可视化结果解释。
- 对比简单的单一模型，集成学习模型可以防止过拟合。GBDT的最大好处在于，每一步的残差计算其实变相地增大了分错实例(instance)的权重，而已经分对的实例(instance)则都趋向于0。这样后面的树就能越来越专注那些前面被分错的实例(instance)。 
- 对于树类型的模型，不需要对数据做预处理，不需要归一化，不需要标准化数据，也不需要满足什么数据前提假设。能灵活的处理各种类型的数据。
- 集成学习模型的非线性变换比较多在构建一系列的树的时候，表达能力强，而且不需要做复杂的特征工程和特征变换。
- 可解释性强，可以自动做特征重要性排序。因此常作为发现特征组合的有效思路。
- 可以解决几乎所有单一树模型的缺点。又同时集成了所有单一树模型的优点。
- 根据不同数据的分布和task类型，可以用不同类型的loss函数来针对异常值，达到对于异常点的容错能力好，模型更robust。。
- 可以处理多维度输出的分类问题，及多分类问题。
- 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
- 在相对较少的调参时间下，预测的准确度较高。
### 缺点
- 随机森林的训练效率会高于GBDT，因为在单个决策树的构建中，GBDT使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。
- 对比随机森林，RF的起始性能较差，特别当只有一个基学习器时，但随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。RF一般降低variance，而GBDT一般降低bias。
- 工程上Boost是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征，如果feature个数太多，每一棵回归树都要耗费大量时间，这一点上大大不如SVM。
### 知识点提炼
- 参数解释
  - n_estimators，构建多少个并行的DT，一般来说GBDT每棵树都很简单，所以总共多少棵树一般选择比较大的值比如200，然后配合early stopping防止过拟合。
  - max_depth，单个决策树的最大深度，和减枝有关，如果不设置，则一直build树到底。这个参数很重要对于GBDT，因为我们会构建很多树，所以单个树越简单越好来避免过拟合，一般设置6左右。
  - min_samples_split，单个决策树分裂后子树最小需要多少个样本才可以去分裂一个树节点。
  - min_samples_leaf，单个决策树节点构建叶子结点的时候，叶子结点最少需要多少个样本来构建。如果分裂后低于这个参数则意味着不再分裂此节点。此参数可能会影响回归任务的平滑效果。
  - min_impurity_decrease，单个决策树节点最小需要降低多少impurity，如果分裂后低于此参数则意味着不再分裂此节点。此参数可以达到early stopping的效果，防止过拟合太多的单一决策树。
  - learning_rate，每个单一决策树对于最终结果的贡献权重，缩小单一树对于最终结果的共享。和n_estimators参数是一组相互牵制的参数，相互之间有trade-off。
  - loss，每次拟合优化的损失函数，对于不同的任务和不同的数据分布，我们应该采用不同的损失函数。
  - tol，early stopping的忍受度，如果loss下降幅度在几轮构建DT中都低于此参数的时候，我们停止训练，及early stopping。
- 损失函数解释
  - 回归问题：
    1. 均方差损失：(y-f(x))^2，适用于大部分数据分布均匀的情况。
    2. Huber损失：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。
    3. 分位数损失：对训练集进行分段预测的时候，则采用quantile。
  - 分类问题：
    1. 指数损失：exp(-yF(x))，类似Adaboost。
    2. 对数损失：交叉嫡损失，主要用于二分和多分类问题。
- 正则化解释
  - Shrinkage，对应参数里面的learning_rate，及我们每次对每棵树的拟合进行缩放。实践中一般把此参数设置很小，比如0.01之类的，然后配合early stopping来使用。
  - Subsample，子采样，取值为 (0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但会增加样本拟合的偏差，因此取值不能太低。推荐在 [0.5, 0.8]之间。
  - Early Stopping，对应参数里面的tol，机器学习迭代式训练模型中很常见的防止过拟合技巧，具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。在sklearn的GBDT中可以设置参数n_iter_no_change和tol实现early stopping。
  - Dropout，此方法比较新，每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble，同时新加的树结果要规范化一下。主要是想平衡一下所有树对预测的贡献，因为会出现前面迭代的树对预测值的贡献比较大，后面的树会集中预测一小部分样本的偏差的情况。
  - 其它正则化的方法就是对每次拟合的单一CART树进行限制，此处参考decision tree里面的note详细介绍了单一树的正则化。
- 其它要点 
  - 子采样，借用bootstrap的思想，每一轮训练时只使用一部分样本，不同点是这里的采样是无放回抽样，这个方法被称为Stochastic Gradient Boosting。对于单棵树来说，只使用一部分样本拟合会增加单棵树的偏差和方差，然而subsampling会使树与树之间的相关性减少，从而降低模型的整体方差，很多时候会提高准确性。subsampling的另一个好处是因为只使用一部分样本进行训练，所以能显著降低计算开销。
  - 大部分单一树的要点可以参考decision tree里面的note。
  - 总的来说，对于GBDT build的时间复杂度一般是O(m * n * log(n) * depth * n_estimators)，predict的时间复杂度是O(log(n) * n_estimators)，m是feature个数，n是样本个数，也就是每科单一树的时间乘以总共几棵树。
  - GBDT对比RF：
    - 1）集成的方式：随机森林属于Bagging思想，而GBDT是Boosting思想。
    - 2）偏差-方差权衡：RF不断地降低模型的方差，而GBDT不断地降低模型的偏差。
    - 3）训练样本方式：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本，此处现在可以通过子采样来同样达到部分样本训练。
    - 4）并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)。
    - 5）最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合。
    - 6）数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感。
    - 7）泛化能力：RF不易过拟合，而GBDT容易过拟合。
  - 多分类问题，本质上是K个class的2分类问题，每次把一个class当做1其它的当做0，每次拟合K个类别的DT在每次迭代过程中，对K个输出做softmax转化成概率然后继续下一轮拟合K个负梯度。用softmax作为损失函数算负梯度同样等价于残差。详细复现需以后补充。
### Engineer Work
- 一般来说直接调用sklearn中的GBDT包来实现。实际生产中普通版本目前来说在(150, 4)的数据集上每棵树拟合需要6s左右在8cores/16Memory上。
- 如需要再1M以上的数据跑production，一般不推荐此算法，model会变的很容易让整串树build的很长很复杂并且很慢，容易导致过拟合。推荐升级版的集成学习比如XGB/LGB。
- 如果需要一定跑大数据，推荐参考spark版本的regression和classification。[参考link](https://spark.apache.org/docs/latest/ml-classification-regression.html)


### 面试问题总结
1. 残差 = 真实值 - 预测值，明明可以很方便地计算出来，为什么GBDT的残差要用用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？
   - 在GBDT中，无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，残差只是特例。
   - GBDT的求解过程就是梯度下降在函数空间中的优化过程。在函数空间中优化，每次得到增量函数，这个函数就是GBDT中一个个决策树，负梯度会拟合这个函数。要得到最终的GBDT模型，只需要把初始值或者初始的函数加上每次的增量即可。详细推理过程可以参考[链接](https://mp.weixin.qq.com/s/Ods1PHhYyjkRA8bS16OfCg)